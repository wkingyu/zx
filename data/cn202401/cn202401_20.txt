基于无线光通信的非正交多址接入技术研究 李 亮  等 热点专题
中兴通讯技术
2024  年 2 月    第 30 卷第  1 期   Feb . 2024    Vol . 30  No. 1策问题 。
4.1 系统模型和问题表述
对于通信信道 ，考虑无人机和地面用户之间的 LoS链
路，无人机和用户 Ut,k之间的信道增益可以表示为[11]：
ht，k=ì
í
îï
ïïï
ïï
ï
ïïï
ïïA()m+1
2πd2
t，kg()ψt，kTS(ϕt，k)cosm ()ϕt，kcos ()ψt，k   
 0<ψt，k<Ψc
 0，    ψt，k>Ψc ， （1）
其中 ，ψt,k、ϕt,k和Ψc分别是入射角 、辐照度角 、接收器视场
的半角 ， A是探测器面积 ，dt,k是无人机到用户 Ut,k的距离 ，
TS(ϕt,k)为滤光片增益 。g(ψt,k)表示光学集中器的增益 ，当
0≤ψt,k≤Ψc时，g(ψt,k)=n2
r
sin2 Ψc，否则为 0。m与发射器半
发散角Φ1/2有关 ，并且m=-ln 2/ln (cos Φ1/2)。
我们将总带宽 B平均划分为 T个子信道 ，对应不同无干
扰小组 。小组内采用 NOMA 方式实现时频资源共享 。考虑
NOMA 中最简单的情况 ，即每小组 2个用户 ，并假设ht,1≥
ht,2。根据 NOMA 的规则 ，功率分配系数 at,1≤at,2，且at,1+
at,2=1，可以得出 at,1≤0.5。则小组t中所有用户的下行可
达速率可以表示为 ：
Rt=
    log2 ()1+ht，1at，1Pt
n0
Rt，1+
            log2 ( )1+ht，2at，2Pt
n0+ht，2at，1Pt
Rt，2 。 （2）
在给定功率 Pmax时，以最大化所有用户的总速率为目
标，优化问题可以数学表示为 ：
max
w，T，α ∑t Rt
 s.t. ht，1≥ht，2
      at，1≤0.5
      Rt，k≥Rth
w∈D
Pt≥0
∑t∈T Pt≤Pmax， （3）
其中 ，w=(xu,yu,zu)是无人机位置 ，T为用户分组策略 ，α=
{a1,1,a1,2,…,aT,1,aT,2}为功率分配系数 。优化{w,T,α}受到如下
多个约束 ：NOMA 功率分配规则 ；Rth是所有用户满足服务
质量 （QoS）条件的最小速率要求 ；无人机限制在 D区域内
飞行 ；每小组t的功率非负且总和不超过总功率 Pmax。该问题非凸且 NP-hard ，传统优化算法很难求解 。此外 ，VLC 信
道模型形式也导致传统凸近似算法不适合求解 。因此 ，我们
采用深度强化学习来解决该问题 。
4.2 深度强化学习算法
在强化学习中 ，环境通常被建模为马尔科夫决策过程 ，
包括状态空间 、动作空间 、奖励 、转移概率分布和折扣因
子。公式 （3）中的优化问题可转化为强化学习环境 。
状态空间包括无人机位置 、用户的信道增益 、用户通信
速 率 、总 通 信 速 率 ，可 表 示 为 ：S=
[w,h1,1,h1,2,…,ht,1ht,2,R1,1,R1,2,…,Rt,1,Rt,2,∑t∈T  Rt] 。
动作空间包括离散和连续两种类型 ，离散动作包括用户
分组 ，所有分组的可能性为 K；连续动作包括无人机位移和
功率分配系数 ，可表示为 xk=(vx,vy,vz,a1,a2,…,aT)。
奖 励 函 数 与 总 通 信 速 率 正 相 关 ，可 表 示 为 R=
∑t∈TRt-C，其中 ，C为一个常数 ，用来控制奖励值在一个
合理的范围 。
在算法层面 ，我们提出了基于多通道深度 Q网络 （MP-
DQN）[17]的混合动作空间的强化学习 （HA-DRL ）算法来分
别处理离散和连续的动作 。为了体现混合动作空间的优势 ，
我们还与单一连续动作空间的深度确定性策略梯度算法
（DDPG ）[18]进行比较 。
4.3 结果与分析
经过 2 500个回合的训练后 ，我们将保存 HA-DRL 和
DDPG 的模型参数并测试 。测试结果如图 5所示 ，可以看出 ，
当用户数量由 4增加到 6后，HA-DRL 相较于 DDPG 算法的
图5 不同强化学习算法的测试曲线比较DDPG ：深度确定性策略梯度算法
HA-DRL ：基于 MP-DQN 的混合动作空间的强化学习算法0 10 20 30 40 50
步骤速率20
18
16
14
12
10N=6，HA-DRL
N=6，DDPG
N=4，HA-DRL
N=4，DDPG
16