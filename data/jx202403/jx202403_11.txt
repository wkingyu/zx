092024/NO.03
中兴通讯技术 ( 简讯 ) 
的效率。
因此，作为智算存储，首先需要提供多元数
据存储能力以及块（ iSCSI）/文件（ NAS）/对象
（S3）/大数据（HDFS）多协议互通能力。可通
过软硬件综合调优来提升性能，硬件加速手段包
括：通过DPU卸载存储接口协议以及去重/压缩/
安全等操作，数据按热度自动分级及分区存储；
软件调优手段包括分布式缓存、并行文件访问系
统/私有客户端等。同时，采用NFS over RDMA以
及GPU直接存储（GDS）技术也能够大大降低数
据访问的时延。
无损网络
AI大模型训练的并行计算特性带来大量通信
开销，使得网络成为制约训练效率的关键因素，
无损网络成为刚需，具体表现为零丢包、高吞吐
大带宽、稳定低时延以及超大规模组网。
目前的无损网络协议主要分为英伟达的IB与
RoCE两大类。 IB网络最初为高性能计算（ HPC）
设计，具备低延迟高带宽、SDN化拓扑管理、拓
扑组网丰富以及转发效率高的优势，但存在产业
链封闭的问题。 RoCE为统一承载网络设计，具备
高带宽/高弹性组网，对云化服务支持较好以及
生态开放的优势，是国产化的必选之路。但是在
网络性能和技术成熟度方面不如IB，需要结合芯
片进一步优化时延。
传统网络拥塞和流量控制算法端侧和网侧独
立，网络仅提供粗颗粒度的拥塞标记信息，很难确
保网络高吞吐满负荷场景下不出现拥塞、丢包以及
排队时延。因此需要端网协同实现精准、快速的拥
塞控制和流量调度算法，进一步强化网络性能。
在网络拓扑方面，Fat-Tree CLOS和 Torus轨道
多平面拓扑为当前两种主流形态，从组网设计上
解决网络拥塞问题。Fat-Tree CLOS网络基于传统树
型网络增强，采用上下行带宽1 1低收敛比，保障
任意两个节点间无阻塞路径；Torus轨道多平面网
络将不同服务器相同位置GPU连接到同一组交平均每两年翻240倍的速度增长，与之相比，AI
内存容量仅以每两年翻2 倍的速率增长，已经远
远不能匹配大模型增长速率。为解决该问题，内
存统一寻址的“超级节点”是目前比较可行的方
案，如：定制AI 服务器，通过高速互联技术组成
1个超级节点（包含256颗 GPU和 256颗 CPU），
支持GPU和 CPU之间的内存统一寻址，内存容量
可以提升230倍。此外，AI芯片内采用计算和存
储分离的冯·诺依曼架构，芯片60%~90%的能量
消耗在数据搬移过程中。按照H800的最大功耗
700W的 60%来估算，数据搬移消耗了 420W。为
解决该问题，存算一体技术将内存与计算完全融
合，避免数据搬移，大幅提升了能效。
互联总线方面，大模型 3D并行拆分后，带
来了芯片间数据传输的要求。其中数据传输量最
大的张量并行（TP），在传输时间中的占比超
90%。有测试数据表明，使用同样数量的服务器
训练GPT-3，采用NVLink相比PCIE，一个
Micro-batch在相邻GPU之间的传输时间从
246.1ms降低到78.7ms，整体训练时间从 40.6天
降低到22.8天，因此互联总线的带宽成为关键。
智算存储
在大模型开发端到端的多个环节中，都对存
储提出了创新需求。具体包括：
多元存储：视频、图像、语音等多模态数据
集带来块、文件、对象以及大数据等多元存
储以及协议互通的要求；
海量存储：为保证大模型训练的精准性，数
据集通常为参数量的2~3倍，在当前大模型
从千亿到万亿飞速发展的时代，存储规模是
一个重要的指标；
并发高性能：大模型并行训练场景下，多个
训练节点需要同时读取数据集。在训练过程
中，训练节点需要定时保存检查点（check-
point）以保障系统的断点续训能力。这些
读写操作的高性能能够大大提升大模型训练