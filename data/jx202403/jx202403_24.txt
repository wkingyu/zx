22
专  题 智能算力
中兴通讯算力及核心网
硬件总工高振中
多样化的AI芯片
1长。以OpenAI为例，2022年训练一次1750亿参
数的GPT-3模型大概需要1024块 A100 GPU运行34
天，2023年训练一次1.8万亿参数的GPT-4模型大
概需要25000块 A100运行约100天。相比GPT-3，
GPT-4的训练时间增加近2 倍。
推理芯片的需求呈现多样化的趋势，主要由
业务场景决定。如线上问答场景，AI芯片的算力
需要跟上人类的阅读速度（平均每分钟阅读250
个单词，最大1000个单词）。如5GC的新通话场
景，需要在AI算力的基础上，叠加语音编解码和
图像处理能力。
AI芯片的部署位置
AI芯片主要部署在云侧和端侧。云侧一般指
云端数据中心，端侧一般指个人可接触或使用，
不需要远程访问的设备内（如手机、PC等）。
云端数据中心，以运营商的智算中心为例，
可进一步细分为集团节点、中心节点和边缘节点
（见图 1）。集团节点用于智算运营管理；中心
节点用于训练和非实时推理；边缘节点与边缘云
混合建设，用于实时推理。
端侧AI 具备安全性、独立性、低时延、高可
靠性等特点，能很好地完成各类AI推理任务。目
前，多个大模型均已推出“小型化”和“场景
化”版本，其轻量化提供了端侧运行的基础。
AI芯片的技术路线
以GPU为代表的通用并行计算架构以及以针956年，在美国达特茅斯学院的夏季研
讨会上，麦卡锡、明斯基等科学家首
次提出AI概念。此后的60余年中，AI发
展历经多次沉浮，经历了漫长的探索期。直到
2015年，AI的视觉识别精度超越人类，开始在视
频领域规模商用。2022年，现象级产品ChatGPT横
空出世，推动大模型成为产业应用的主要方向。
AI芯片是AI发展的重要基石，经历了两个主
要阶段。2012 年以前，AI 研究和应用主要基于
CPU； 2012年，多伦多大学的Alex Krizhevsky首
次将 GPU用做AI，只用了 4颗英伟达 G eforce 
GTX580（同时期的谷歌方案采用 16000颗 CPU） 
就在 ImageNet竞赛中获得冠军，震撼了学术界，
开启了AI芯片多样化的大门。
AI芯片的关键需求
从功能上，AI 芯片可分为训练芯片和推理芯
片两大类。训练，指的是通过向模型提供大量标
注或未标注的数据，并基于优化算法来调整模型
的参数，使其能够从数据中学习到相关的模式和
规律。推理，是指将已经训练好的模型应用到实
际场景中，进行预测、分类或决策。
训练芯片的关键需求是如何提供更高的AI 算
力，降低模型的训练时间。大模型趋势发生以
来，模型的数量、规模，在短短几月内剧增，百
亿千亿级别大模型飙升至数十个，万亿参数大模
型已正式诞生。训练所需的计算量也随之呈指数
增长，且翻倍时间约三四个月，远快于芯片工艺
的摩尔定律，导致大模型的训练时间不断被拉