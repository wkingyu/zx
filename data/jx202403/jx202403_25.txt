图1  运营商的智算中心部署
       架构
232024/NO.03
中兴通讯技术 ( 简讯 ) 
中心
（大区/省份）
边缘
（地市/区县/现场）智算核心节点
（单独建设） 网络云/移动云/IT云
训练+推理
实时推理 实时推理边缘云
边缘推理节点边缘云
边缘推理节点AI芯片 AI芯片
AI芯片 AI芯片 AI芯片通用业务
实时推理边缘云
边缘推理节点集团
（全国）
智算运营管理系统训推一体 多云编排智算运维 运营管理编排调度
训练请求
数据采集
模型传递
大，从千卡演进到万卡。功耗不断增加，需要引
入液冷才能满足散热要求。另一方面，模型在端侧
落地也面临着功耗问题，AI手机和AI PC作为两种典
型的端侧设备，功耗的增加都会影响消费体验。针
对以上问题，下一代AI芯片设计有以下方向：
在计算架构层面，引入存算一体架构，降低
功耗。当前的主流GPU 和AI专用芯片均采用冯·
诺依曼架构，计算和存储分离，芯片60%~90%的
能量消耗在数据搬移过程中。存算一体架构将内
存与计算完全融合，避免数据搬移，可大幅降低
功耗。
在芯片实现层面，采用Chiplet和 3D堆叠技
术，提升芯片良率和性能。Chiplet将芯片分割成
多个具有特定功能的芯粒（如计算芯粒、存储芯
粒等），各种芯粒选择最适合的半导体制程进行
分别制造，实现最优的良率，再通过高速总线将
彼此互联，最终集成封装为一颗芯片。3D堆叠
把芯片从二维展开至三维，在不改变原本的封
装体积大小的基础上，通过在垂直方向进行芯
粒叠放，增加芯片内的芯粒数量，进而提升芯
片性能。对AI领域加速为代表的专用定制架构，是目前两
大主流AI芯片技术路线。
GPU设计初衷是进行图形渲染。图形处理涉
及到相当多的重复计算量，因此GPU芯片上排布
了数以千计专为同时处理多重任务而设计的图像
计算核心，正好和AI运算的数据量规模大、可并
行的特点相匹配。
不同于GPU， AI专用芯片是一种针对AI 运算
的专用处理器，内部以 AI专用核为主，相比
GPU，减少了视频渲染、高性能计算等功能。AI 
专用芯片在功耗、体积等方面有一定的优势，但
由于是专用定制的设计思路，开发周期较长，在
通用性和可编程性方面也弱于GPU，整体处于多
而不强的局面。
未来展望
万亿大模型已成为事实，不远的未来很可能
出现十万亿的超大模型。随着模型规模的不断增
长，无论是GPU还是AI 专用芯片，性能和功耗都
出现了瓶颈，导致云端数据中心的规模不断增