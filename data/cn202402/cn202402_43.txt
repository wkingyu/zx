基于存算一体集成芯片的大语言模型专用硬件架构 何斯琪  等 热点专题
中兴通讯技术
2024  年 4 月    第 30 卷第  2 期   Apr . 2024    Vol . 30  No. 2这种情况表明 ，随着大型模型的不断发展和应用场景的
扩大 ，现有的硬件架构在满足大规模模型计算需求方面面临
着巨大的挑战 。具体而言 ，参数量巨大且算力要求高的大模
型导致了计算和存储资源高需求的问题 ，而当前的 GPU/
TPU+DRAM 结构的带宽限制使得数据传输方面的瓶颈日益
显现 。因此 ，未来的硬件设计和架构需不断创新 ，以适应快
速增长的大型模型计算需求 ，提供更高效的数据传输和处理
解决方案 。
2 存算一体集成芯片的优势
2.1 缓解带宽瓶颈
经典存算一体的设计基于交叉阵列 。根据欧姆定律和基
尔霍夫定律 ，输入特征用存储阵列的字线上的电压表示 ，输
出特征会表示为位线上的电流大小 ，因此能够一次性完成矩
阵乘加操作 。同时 ，由于在计算过程中仅进行输入输出的搬
运，权重系数一直固定在存储阵列中 ，所以能够显著减少数
据搬移开销 。我们发现 ，如果采用 CPU+ 存算一体的组合的
架构 ，相较于现有的 GPU/TPU + DRAM 分离计算架构 （如图 4
所示 ） ，能够在相同的令牌速率和算力下 ，实现带宽的显著
节约 ，达到 xPU+HBM 架构下 1 000+倍的水平 。举例来说 ，当
采用和第 1节相同的令牌速率 （10 000个/s）时，存算一体架
构仅需 32～64 Gbit/s 的带宽 ，就能节省超过 1 000倍的带宽 。
另一方面 ，当单颗芯粒的算力达 10 TOPS ，存储容量达
到200 MB时，根据 12/14 nm工艺估算 ，芯粒的计算电路面
积约为 8 mm²，存储面积约为 300 mm²，此时实际的算力密
度大约为 0.032 5 TOPS/mm2。因此 ，存算一体集成芯片架构
相对于传统的数据中心系统不仅在性能上取得了显著的提
升，还在所需的单芯粒接口速度远低于现有管控指标的前提
下，为大规模模型的计算提供了更为可行的解决方案 。2.2 存边架构高并行度数据流
以图 3所示 LLAMa 模型为例 ，我们对大模型全连接层算
力和存储容量进行分析 ，其三层连续的全连接层网络的算力
需求为 ： （4 096×11 008 + 11 008×4 096 + 4 096×4 096）×
32×10 000 ×2 ≈ 68 TOPS ；存储容量为 ： （4 096×11 008 + 
11 008×4 096 + 4 096×4 096）×32 ≈ 3.4 GB，即模型的算
力需求与存储容量的比值为目标令牌速率 ，与网络大小无
关。在数据中心中 ，令牌速率约为 1～10 000个/秒，经典的
卷积神经网络模型 ResNet- 50的算力与存储比为 4.1×帧率  
GOPS/ 25 MB = 164×帧率 （GOPS/kB )） ，因此大模型的算力
存储比远低于以卷积神经网络 （CNN）为主的传统深度神经
网络 （DNN）模型的算力存储比 。传统交叉阵列架构算力存
储比为时钟频率 ×2。为适应大模型的算力存储比 ，我们提
出了存边计算架构 （COMB ） ，即将乘加计算逻辑分布在片
上权重缓冲静态随机存储器 （SRAM ）的边缘 ，算力存储比
图3 LLAMa- 7B模型全连接层和注意力模块参数维度示意图FC：全连接层         TOPS ：每秒执行一万亿次计算
图4 存算分离和存算一体架构对比CPU：中央处理器
DDR ：双倍速率同步动态随机存储器
DRAM ：远程直接内存访问
GPU：图形处理器HBM ：高速带宽存储器
NPU：网络处理器
PCIE ：高速串行计算机扩展总线标准
RDMA ：远程直接数据存取（a）现有 GPU/TPU+ DRAM 分离计算架构
（b）CPU+ 存算一体架构全连接层矩阵尺度 LLAMa 2-7B FC 计算
注意力输出 注意力输出解码器掩码
逐行归一化指数函数
概率矩阵激活层归一化
多头注意力
注意力输入K、Q、V
Generation
4 096×11 088
层数：32层
总参数量 ：6.51B
总算力 ：2.5 TOPS4 096×11 088 4 096×4 096
4 096×11 088
4 096×4 096×3Score （S）=Q×KT1010
FC0 FC1FC2
Gate
FCgK·FC
Q·FC
V·FC
CPUPCIEGPU
加速卡先进工艺
GPU 芯片2.5D封装3D封装
DRAM
（HBM ）
CPUPCIE
RDMA 存算一体
加速卡DDR矢量 CPU/
小算力 NPU存算一体
存算一体DRAM……
……
39