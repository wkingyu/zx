大模型关键技术与应用 韩炳涛  等 企业视界
中兴通讯技术
2024  年 4 月    第 30 卷第  2 期   Apr . 2024    Vol . 30  No. 2解码器 。其中 ，编码器负责接收对应模态信息的输入 ，并将
其编码到语义空间的向量中 ；解码器则负责从语义空间的向
量中解码出对应模态的输出 ；跨模态对齐单元则负责不同模
态语义空间的匹配对齐 。这种架构可以很方便地实现多种模
态的混合输入和输出 。
1.4.1 多模态理解大模型
目前 ，视觉语言类的多模态理解大模型和多模态生成大
模型的发展最为迅速 。视觉语言理解大模型 （图生文 ） ，即
对视觉编码器 +模态对齐 +大语言模型解码器进行组合训练 。
代 表 性 的 工 作 包 括 CLIP[13]、BLIP- 2[14]、LLaVA[15]和
InternLM-XComposer 2[16]等。
BLIP- 2由预训练好的 、冻结参数的视觉模型 （CLIP 训
练 的 ViT-L/ 14、EVA-CLIP 训 练 的 ViT-g/ 14） 、 文 本 模 型
（OPT、FlanT 5） ， 以及所提出的可训练的 Q-Former 构成 。
Q-Former 是一个轻量级 Transformer ，它使用一组可学习的
Query 向量 ，从冻结的视觉编码器中提取视觉特征 ，来对齐
文本和语言两个模态的差距 ，从而把关键的视觉信息传递给
大语言模型 （LLM） 。
LLaVA 成功地验证了少量高质量的数据能使模型拥有很
强的图文生成能力 ，其由 3部分组成 ：CLIP 预训练模型中的
视觉编码器 ViT-L/ 14、一个线性投影层和一个大语言模型
LLaMA 。LLaVA 以图 -文对 （LAION 、CC3M、COCO ）数据
集 为 基 础 ，使 用 ChatGPT/GPT- 4来 构 建 指 令 跟 随 精 调 数
据集 。
零一万物开源 Yi-VL 多模态大模型[17]也是采用 LLaVA 架
构，使用了 Yi-34B-Chat 模型 ，改进了微调训练方法 ，提高
了Yi-VL 无缝集成和解释视觉 +语言多模态输入的能力 。
1.4.2 多模态生成大模型
视觉语言生成大模型 （文生图 ）的解码器部分以扩散模
型 为 主 ，代 表 性 工 作 包 括 Stable Diffusion[18]、DiT[19]、
Sora[20]等。
Stable Diffusion 的组件和模型组成为 ：文本编码器 ，将
文本信息转换成数字表示 ，以捕捉文本中的想法 ；图像信息创建者 ，在隐空间中逐步处理扩散信息 ，以文本嵌入向量和
由噪声组成的起始多维数组为输入 ，输出处理的信息数组 ；
图像解码器 ，使用处理后的信息数组绘制最终的图像 。
Sora是OpenAI 发布的文生视频的多模态模型 ，和Run‐
way、Stable Video Diffusion 及PIKA 等已有模型相比 ，其视频
生成能力有大幅提升 。Sora 模型内部分成 3个部分 ：第1部
分是变分自动编码器 （VAE） ，包括编码器和解码器两个部
分，其作用是对视频进行压缩和解压缩 。生成视频的过程是
在压缩后的低维隐空间进行计算 ，相对于直接从原始的像素
空 间 计 算 ，减 少 了 数 百 倍 的 计 算 量 。第2部 分 是 基 于
Transformer 的扩散模型 ，其作用是在隐空间通过迭代降噪过
程生成视频 。第3个部分是语言大模型作为编码器 ，其作用
是将用户的 Prompt 编码为一个隐空间的表示 ，使其在生成
视频时内容与文本描述一致 ，从而使视频内容和用户的
Prompt 一致 。
2 模型训练
GPT 开创了生成式预训练方法之后 ，两阶段训练 （任务
无关的预训练阶段和任务相关的精调训练阶段 ）大模型成为
主流 。预训练阶段的目的是为模型注入大量通识知识 ，精调
训练阶段的目的是提升模型完成特定任务的能力 。在这种方
式下 ，仅需少量精调数据即可以让预训练模型具备完成新任
务的能力 ，相比于之前为每个任务端到端完整训练模型 ，极
大节省了训练数据和算力 。
随着开源预训练大模型的出现以及应用场景日益复杂 ，
上述两阶段训练方法已不再满足需求 ，因此出现了更多的训
练阶段 ，如图 2所示 。由于开源预训练大模型通常使用公开
可获得数据训练 ，专业领域知识不足 ，使用私域数据对模型
再次进行增量预训练可以有效灌注专业知识 。大模型在使用
过程中需要避免生成各类有害信息 ，因此在模型完成任务相
关的精调训练之后 ，增加了对齐训练阶段 ，这样可以使模型
输出更加符合人类的价值观 。
训练高性能的商用大模型 ，涉及数据处理 、预训练 、精
调训练 、安全等关键技术 。
图2 大模型多阶段训练过程预训练 增量预训练 精调训练 对齐训练
基础 LLM 领域 LLM 场景 LLM 评价模型
通识数据集 领域数据集 领域数据集
LLM：大语言模型
78