大语言模型算法演进综述 朱炫鹏  等 热点专题
中兴通讯技术
2024  年 4 月    第 30 卷第  2 期   Apr . 2024    Vol . 30  No. 2减少了大量计算 ，但也带来了存储和带宽的压力 。例如 ，
GPT- 3模型参数占用显存大小为 350 GB，假设 Batch 大小为
64，输入序列长为 512，输出序列长度为 32，则KV cache 占
用显存为 164 GB，大约是模型参数占用显存的 1/2。KV 
Cache 的规模较大 ，且对存储带宽要求较高 ，并会遭遇内存
墙问题 。因此 ，又出现了多查询注意力 （MQA ）[25]和分组查
询注意力 （GQA）[26]。
3.2.2.2 共享关注头
在原始 Transformer 的MHA 中，QKV 分别包含相同数量
的头 ，且一一对应 。每个头的 QKV 内部进行计算 ，再将结
果拼接在一起 。
MQA 的QQ仍然保持原来的头数 ，但KK和VV只有一个头 ，
共享给所有 QQ头使用 ，如图 3（c）所示 。MQA 免除了多个
KVKV头的计算和存储 ，大大减少了存储和访存带宽压力 ，推
理吞吐量可提高 30%～40%，而模型性能只有少量损失 。
GQA 是MHA 和MQA 的折衷 ，该方法减少了模型性能损
失，获得 MQA 带来的推理加速好处 。具体方法是 ，不是所
有QQ头共享一组 KVKV，而是 QQ头分成多组 ，每组共享一组
KVKV，例如图 3（b）是每两个 QQ头共享一组 KVKV。
3.2.2.3 推测解码
推测解码[27-28]是2023 年新兴的大语言模型推理加速技
术，通过增加推理的并行度来提高计算效率 ，降低延时 。其
具体方法是为大语言模型配备一个小语言模型 ，推理时 ，先
由小语言模型 “推测 ”生成几个 tokens ，然后将这几个 to‐
kens 放入大语言模型中进行推理验证 。如果验证正确 ，则小
语言模型继续 “推测 ”后续 token ；如果验证错误 ，大语言
模型修正已有 token ，小语言模型接受修正 ，继续 “推测 ”
后续 token 。
推测解码之所以能降低延时 ，一是因为小模型计算速度
远超大模型 ，有数量级的提升 ；二是因为大模型并行推理几个tokens ，只需读取一次参数 ，计算强度提高 ，平均每个
token 的计算延时大幅降低 。
3.2.2.4 精简 Transformer
文献 [29]介绍了 Transformer 的简化 ，并以信号传播理论
及实证研究结果为基础 ，证明了 Transformer 中许多组件 ，如
残差连接 、Value 、投影和 LayerNorm ，可以在不牺牲训练速
度的情况下被删除 。在纯自回归解码器和纯 BERT 编码器模
型上的实验表明 ，简化后 Transformer 实现了与标准 Trans ‐
former 相当的训练速度和性能 ，同时训练吞吐量提高了
15%，使用的参数减少了 15%。
3.2.3 访存优化
3.2.3.1 FlashAttention
FlashAttention[30]通过分块计算 Softmax 和核函数融合 ，来
降低对显存的访问 。
计算 Softmax 需要遍历两遍全体数据 。FlashAttention 修
改了 Softmax 算法 ，将数据分成多个小块 ，无须遍历即可逐
块计算出 Softmax 的中间结果 ，当所有块计算完成后 ，再对
中间结果进行一次校正 ，就可得最终结果 。
FlashAttention 还将 Softmax 和前后的矩阵乘等算子融合
成一个统一计算设备架构 （CUDA ）核函数 。每块数据在
GPU 上完成核函数计算后 ，结果被输出到显存 。这个过程充
分利用了数据局部性 ，中间激活保存在 GPU 缓存 ，避免了
反复读写显存 ，这可以使计算速度提升 7.6倍。
FlashAttention- 2[31]是对 FlashAttention 的改进 ，它消除了
原先频繁的系数更新 ，减少了对加速设备不擅长的非矩阵乘
法运算的需求 ，提出了在序列长度上的并行化 ，获得并行加
速优势 。结合 GPU 运行特点 ，在一个注意力计算块内 ，我
们将工作分配在一个 CUDA 线程 block 的不同 warp 上，以减
少通信和共享内存读写 ，提高模型效率 。
3.2.3.2 Flash-Decoding
Flash-Decoding[32-33]借鉴了 FlashAttention 的思路 ，将并
行化维度扩展到 KV序列长度 。KV序列被分成多个小块 ，每
块内部完成 QKV 注意力计算 ，多块之间可并行计算 ，无须
等待 Softmax 统计全局最大值 。
Flash-Decoding 不是在运行时实时统计数据最大值 ，而
是在模型设计时离线统计数据的分布 ，在分布区间内取一个
较大的值 Φ作为最大值 。运行时直接用 Φ计算 Softmax ，可
以应付 90%以上的情况 。如果实际的最大值超过了 Φ，就会
 图3 MHA 、GQA 和MQA[26]MHA GQA MQA
VV
KK
QQ
（a） （b） （c）
GQA ：分组查询注意力      MHA ：多头注意力     MQA ：多查询注意力
14