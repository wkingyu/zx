通信网络与大模型的融合与协同 任天骐  等 热点专题
中兴通讯技术
2024  年 4 月    第 30 卷第  2 期   Apr . 2024    Vol . 30  No. 2数万亿个参数 。此外 ，通过预训练和微调 ，
大模型中还融合多模态技术[9-10]，使得大模
型在自然语言处理 、计算机视觉[11-12]、自动
驾驶[13-14]等多个领域展现出强大的潜力 。
同时 ，数据 、算力与模型构成了实现
AI的三大基石 ，而6G成为 “通、感、算、
智、存”集于一体的超级基础设施平台 ，为
融合 AI提供了充足的条件 ，因此基于内生
智能的新型网络架构应运而生 。内生智能网
络不仅要引入 AI来构建网络 ，还需要充分
利用网络节点的通信 、计算和感知能力 ，并
将通过分布式学习 、群智式协同以及云边端
一体化算法部署 ，原生支持各类 AI应用 ，
为各行业用户提供实时 AI服务和实时计算
类新业务[15-16]。
本文中 ，我们将首先探讨大语言模型
（LLM）的基础原理 ，包括 Transformer 结构 、
标度率和涌现能力 ，以及 LLM 的预训练与
微调过程 。进一步地 ，我们将分析 AI，特
别是 LLM 在通信网络中的应用及其带来的
双向增益 。同时 ，也将探讨大模型发展面临
的问题与挑战 ，以及如何更好地利用 AI技
术来优化并实现通信网络的转型 。
1 大模型的理论与技术
1.1 大模型架构
现 有 LLM 的 进 步 主 要 得 益 于 Transformer 的 发 展[1]。
Transformer 模型完全摒弃了传统语言模型广泛使用的循环神
经网络 （RNN）和长短期记忆网络 （LSTM ）模型 ，全面采
用自注意力机制来处理序列 。
如图 1所示 ，Transformer 模型包含编码器和解码器 ，两
者均由 N个（原文中 N=6）相同的层堆叠而成 。编码器负责
理解输入文本并构造语义表示 ，而解码器则使用编码器的输
出来生成目标序列 。编码器中的每个层由多头自注意力层和
全连接前馈网 （FFN）两个子层构成 ，而解码器相比编码器
多出一个掩码多头自注意力层 。注意力机制的引入使得
Transformer 模型在处理序列的每个元素时 ，能够考虑到整个
序列的上下文信息 ，从而在 NLP 任务中表现出并行化训练和
性能优异的特点[18]。例如 ，Transformers 架构通过自注意力
机制解决了长距离依赖问题 ，使模型能够直接关注到序列中
任意两个位置之间的关系 。同时 ，Transformer 架构允许比RNN 更多的并行化 ，这使得图形处理器 （GPU）上的大量数
据上有效地预训练非常大的语言模型成为可能 。
Transformer 模型的提出极大地推动了 LLM 的发展 。LLM
的发展历程如图 2所示 。基于 Transformer 架构 ，LLM 演化为
3种 主 要 架 构 ：仅 编 码 器 （encoder-only ）、 仅 解 码 器
（decoder-only ） 、编码器 -解码器 （encoder-decoder ） 。目前 ，
最主流的是仅解码器架构 ，代表性的 LLM 有GPT 系列[3，7-8]、
LLaMA[18-19]、PaLM[20]等。仅编码器架构模型的代表是 BERT
系列 ，包括 BERT[2]、RoBERTa[21]和ALBERT[22]等；编码器 -
解 码 器 架 构 的 代 表 模 型 有 谷 歌 的 T5模 型[23]、Meta AI 的
BART 模型[24]和华为的 Pangu 大模型等 。3种架构各有优劣 ：
仅解码器架构更多关注于从已有的信息扩展出新的内容 ，适
合文本生成和扩展类型的任务 ，但需要大量的训练数据来提
高生成文本的质量和多样性 ；仅编码器架构能更好地理解输
入文本的语义和上下文信息 ，适合理解和分析类型的任务 ，
缺点是无法直接生成文本输出 ；编码器 -解码器架构能更好
图1 Transformer 模型架构[1]输出概率分布
Softmax 归一化
线性层
残差&正则化
前馈网络
残差&正则化
多头自注意力
残差&正则化
掩码多头自注意力残差&正则化
前馈网络
残差&正则化
多头自注意力编码器
Nx
输入 输出输入词
嵌入输出词
嵌入位置
编码位置
编码Nx解码器
30