大模型关键技术与应用 韩炳涛  等 企业视界
中兴通讯技术
2024  年 4 月    第 30 卷第  2 期   Apr . 2024    Vol . 30  No. 2数据中的有毒语言或偏见表达 ，确保输入数据的质量和安
全性 。
2）除杂和去标识化 ：从数据集中移除无关的信息和个
人标识信息 ，减少隐私泄露的风险 ，并防止模型学习到不必
要的个人信息 。
上述防御的实现方法可以分为黑盒防御和白盒防御两大
类。黑盒防御通常采用基于查询的模型诊断方法 ，检测模型
是否被嵌入了后门中 。白盒防御包括通过微调删除后门[32]、
模型修剪[33]和通过检查激活来检测后门[34]等方法 。例如 ，
Fine-mixing[35]是一种旨在防止在微调模型中出现后门的方
法。CUBE 防御技术[36]利用了一种称为 HDBSCAN 的密度聚
类算法来准确识别数据集中的簇 ，来区分包含毒害样本和干
净数据的簇 。通过利用 HDBSCAN 的能力 ，CUBE 旨在提供
一种有效区分正常数据和有毒数据的方法 。
3 模型推理和优化技术
大模型以其强大的理解和生成能力 ，正在深刻改变我们
对人工智能的认知和应用 ，但其高昂的推理成本也阻碍了技
术落地 。因此 ，优化大模型的推理性能成为业界研究的
热点 。
推理性能优化主要以提高吞吐量和降低时延为目的 ，关
键技术可以划分为 ：内存管理 、算子融合 、模型压缩 、并行
推理 、服务调度优化 、推理安全及新兴技术 。
3.1 内存管理
KV Cache 是大模型推理
性能优化最常用的技术 。该
技术在不影响任何计算精度
的前提下 ，通过空间换时间 ，
大幅提升推理性能 。Trans ‐
former 解码器使用自回归产
生输出 ，即每次推理只会预
测输出一个 token ，执行多次
后完成全部输出 。前后两次
的输入只相差一个 token ，这
就存在大量重复计算 。KV 
Cache 技术将每个 token 可复
用的 KK和QQ向量结果保存下
来复用 ，将计算复杂度从 O
(n2)降低为 O(n)。
Paged Attention 将操作系
统中的分页内存管理应用到KV Cache 的管理中 ，这节约了 60%～80%的显存 ，从而支持
更 大 的 batch-size ，将 吞 吐 率 提 升 了 22倍。具 体 来 讲 ，
Paged Attention 首先将每个序列的 KV Cache 分成若干块 ，每
个块包含固定数量 token 的键和值 ，然后计算出当前软硬件
环境下 KV Cache 可用的最大空间 ，并预先申请缓存空间 。
在推理过程中 ，通过维护一个逻辑块到物理块的映射表 ，使
多个逻辑块对应一个物理块 ，并使用引用计数标记物理块被
引用的次数 ，从而实现将地址不连续的物理块串联在一起统
一管理 。
RadixAttention 是一种自动键值缓存重用技术 ，该技术
可以在完成生成请求后不直接丢弃键值缓存 ，而是在基数树
中保留提示和生成结果的键值缓存 ，从而实现高效的前缀搜
索、插入和驱逐 。该技术在多轮对话场景可以极大地降低首
字时延 。
3.2 算子融合
算子融合是深度学习模型推理的一种典型优化技术 ，旨
在 通 过 减 少 计 算 过 程 中 的 访 存 次 数 和 统 一 计 算 架 构
（CUDA ）Kernel 启动耗时 ，达到提升模型推理性能的目的 。
以HuggingFace Transformers 库LLaMA- 7B模型为例 ，该
模型有 30个类型共计 2 436个算子 ，其中 aten::slice 算子出现
频 率 为 388次。大 量 小 算 子 的 执 行 会 降 低 图 形 处 理 器
（GPU）利用率 ，最终影响推理速度 。针对 Transformer 结构
图4 Transformer 层中的算子融合示意图FF：前馈
GELU ：高斯误差线性单元MLP ：多层感知机
QKV：查询-键-值QKV
1.5倍加速正则化查询
键
值+
+
+Q-trans
K-trans
V-trans注意力
得分Softmax
归一化
注意力上下文注意力
注意力
输出TransformerAll 
Reduce
+ +
正则化
中间 FF GELU + 输出 FFAll 
Reduce+ +2.9倍加速Q_bias
K_bias
V_bias
Attn_bias
MLP
3倍加速1.2倍加速
Bias-add
O_bias I_bias
81