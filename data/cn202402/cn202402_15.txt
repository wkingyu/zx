大语言模型算法演进综述 朱炫鹏  等 热点专题
中兴通讯技术
2024  年 4 月    第 30 卷第  2 期   Apr . 2024    Vol . 30  No. 2Attention(Q， K， V)=Softmax(QKT
Dk)V=AV，，
（1）
其中 ，Q∈RN×Dk，K∈RM×Dk，V∈RM×Dv，N是Q的长度 ，
M是K的长度 ，Dk是K的向量维度 ，Dv是V的向量维度 。
注意力机制能够并行计算所有 tokens 间的相关信息 ，没
有距离的限制 ，与RNN 、LSTM 相比更具有优势 。
2.1.2 多头注意力
多头注意力 （MHA ）是将Q、K、V转换成多份 ，每份
单独计算注意力 ，结果合并在一起 。每一份称为一个头 ，多
个头可以计算不同领域中的相关关系 ，增加模型的信息容量
和能力 ，具体如公式 （2） ，结构见图 1。
MultiHeadAttn(Q， K， V) = Concat(head1，⋯，headH)WO，
headi=Attention(QWQ
i，KWK
i，VWV
i)，                                   （2）
其中 ，Q、K、V的向量维度都是 Dm，转换后每一份的维度
分别为Dk、Dk、Dv，合并后维度又恢复成 Dm。
在Transformer 中多头注意力有 3种不同的形式 ：
1）自注意力 。多头注意力公式中取 Q=K=V=X，X
是Transformer 的输入特征 ，即计算X与自己的注意力 。
2）掩码自注意力 。在自注意力公式中 ，将注意力矩阵
A中的某些值改为 -∞，避免一些 tokens 间的关注 。
3）交叉注意力 。在编码器和解码器之间计算注意力 ，
K、V来自编码器 ，Q来自解码器 。
2.1.3 前馈网
前馈网 （FFN）由两个全连接层组成 。经过第 1个全连
接层 ，特征维度由 Dm扩大到Df；经过第 2个全连接层 ，特
征维度由 Df恢复到Dm，具体见公式 （3） ：
FFN(H')=ReLU(H'W1+b1)W2+b2， （3）
其中 ，H'是本层输入 ，W1∈RDm×Df，W2∈RDf×Dm，b1∈RDf，
b2∈RDm。
2.1.4 残差连接和归一化层
残差连接能够防止梯度消失 ，归一化层使特征数值维持
在均值 0、方差 1。这样多个 Transformer 组合成深层网络 ，
可以保持前向 、反向数值的稳定 ，具体见公式 （4）—（5） ：
H'= LayerNorm(SelfAttention (X) + X)， （4）
H = LayerNorm(FFN(H') + H')。                                          （5）2.1.5 位置编码
Transformer 一次性输入序列的所有 tokens ，不像 RNN 那
样可以根据输入顺序表示 token 的前后关系 。因此 ，Trans ‐
former 需要在每个 token 上累加一个位置编码 ，来表示 token
在序列中的位置 。
2.1.6 Transformer 整体架构
完整的 Transformer 由编码器和解码器两部分组成 ，结构
如图 1。编码器包括多头自注意力 、前馈网和其他辅助层 。
解码器包括掩码多头自注意力 、多头交叉注意力 、前馈网和
其他辅助层 。
在设计模型时 ，我们根据模型的不同功能 ，可以选择编
码器和解码器的不同组合 。
1）使用编码器 -解码器 。使用 Transformer 的完整结构 ，
输入 、输出都是序列 。此结构一般用于序列到序列任务 ，如
文本翻译 。
2）只使用编码器 。输入为序列 ，输出是序列的表示 。
此结构一般用于文本分类 、序列标记任务 ，如BERT 模型使
用的是编码器 。
3）只使用解码器 。因为只有解码器 ，没有编码器 ，要
移除解码器中与编码器关联的多头交叉注意力 。此结构的输
入为序列 ，输出是一个 token （该token 再作为输入 ，继续输
出下一个 token ，直到输出结束 ） 。此结构一般用于序列生成
任务 。GPT 模型使用的是解码器 。
2.2 ChatGPT 系列模型架构
OpenAI 从2018 年发布 GPT- 1[14]到2023 年发布 GPT- 4，
模型的能力产生了质的飞跃 。模型虽然一直保持 Trans ‐
former Decoder 的总体架构不变 ，但具体模块有所调整 ，如
GPT- 2[15]将LayerNorm 移到解码器的输入 ，而GPT- 4引入了
混合专家 （MoE）层[16]。另外 ，模型的规模也在数量级地增
加，参数量从 1.17×108增加到 1×1012以上 。这样的量变引
起质变 ，模型产生了涌现能力 。
ChatGPT 系列模型的主要创新点和架构如表 1所示 。
3 大语言模型高效推理
3.1 大语言模型的计算特性
Transformer 的结构与 CNN 有较大差别 。CNN 的卷积计
算数据复用率高 。Transformer 中的矩阵乘法 、矩阵乘向量 ，
数据复用率较低 ；非线性算子 Softmax 、LayerNorm 计算时要
多次遍历数据[17]。这些特点造成 Transformer 无法充分利用计
11