大语言模型算法演进综述 朱炫鹏  等 热点专题
中兴通讯技术
2024  年 4 月    第 30 卷第  2 期   Apr . 2024    Vol . 30  No. 2算硬件的能力 ，在现有图形处理器 （GPU） 、加速器上计算
效率较低 。
3.1.1 模型规模大
大语言模型存在计算量大 、存储量大的特点 。以GPT- 3
为例 ，GPT- 3包含 96层Transformer ，每层有 96个注意力头 ，
词向量深度为 12 288。整个模型参数量达到 1 750亿个 ，按
照INT8数据格式计算 ，总大小达到 175 GB。推理生成一个
token 需要的计算量达到 3 240亿次 。
英伟达 A100型号 GPU 的INT8算力为 624万亿次运算每
秒（TOPS ） ，算力利用率小于 10%。A100的显存为 80 GB，
显然无法装下整个 GPT- 3模型 。因此 ，对于 GPT- 3这类大
语言模型 ，推理必须引入分布式技术 ，将一个模型拆分到多
个GPU 上，提升推理速度 ，减小推理延时 。
3.1.2 计算强度低
考虑 GPU 计算深度学习模型的效率 ，我们需要引入算
术强度的概念 。算术强度的含义是模型的计算量与内存读写
量的比值 ，具体如公式 （6）[17]：
Arithmetic Intensity=FLOPs
MOPs ， （6）
其中 ，FLOPs表示浮点计算次数 ，MOPs表示内存访问次数 ，
以一个字节数据为单位统计 。算术强度越高 ，说明模型的计
算密集度越高 ，反之则说明模型的访存密集度越高 。对于特
定的 AI加速器 ，算术强度有一个最佳值 ，模型达到这个最佳
值，就能够同时最大效率地利用存储带宽与计算资源 。如果
高于最佳值 ，模型受限于计算资源 ，造成存储带宽的浪费 ；
反之 ，模型受限于存储带宽 ，则会造成计算资源的浪费 。图2展示了 Bert-base 、Bert-large 、GPT- 2、GPT- 3在不
同序列长度下的推理算术强度 。可以看出 ，由编码器组成的
Bert系列模型算术强度较高 ，达到数百 ，且随着序列长度变
化而变化 ；由解码器组成的 GPT 系列模型算术强度很低 ，只
有2，且不随序列长度变化而变化 。算术强度为 2，意味着
读取一个数据只计算 2次，计算器件的大量时间浪费在等待
数据上 ，无法充分发挥算力 。
表2是Llama 2模型[18]（数据类型 FP16）在不同 Batch
（批量 ）下各层的推理算术强度 。可以看出 ，Llama 2的算术
强度随着 Batch 的增大而增大 。当Batch 为1时，算术强度为
1，推理效率很低 ；当Batch 增加到 512时，算术强度增加到
10.18。再继续增加 Batch 大小 ，算术强度基本不变 ，因为此
时激活的大小已超过权重大小 ，在访存量中占主导地位 。
3.1.3 非线性层计算效率低
Transformer 中包含 LayerNorm 、Softmax 等非线性运算 ，▼表1 ChatGPT 系列模型的主要创新点和架构
模型名
GPT- 1
GPT- 2
GPT- 3
ChatGPT （GPT- 3.5）
GPT- 4主要创新点
• 基于 Transformer 解码器的单向语言模型
• 无监督预训练 +有监督微调模式
• 多任务预训练 ，取消微调
• 将LayerNorm 移到解码器的输入
• 模型层数增加
• 上下文学习
• 少样本学习 、单样本学习 、零样本学习
• RLHF
• PPO
• 引入 MoE
• 多模态发布时
间/年
2018
2019
2020
2022
2023上下文序
列长度
（token ）
512
1 024
2 048
4 096
8 000Transformer
层数
12
48
96
-
-多头
数量
12
48
96
-
-参数量
1.17×108
1.5×109
1.75×1011
>1.75×1011
>1×1012
MoE ：混合专家          PPO ：近端策略优化      RLHF ：人类反馈强化学习
图2 Bert系列和 GPT系列模型的算术强度算术强度 /（FLOP/Byte ）
序列长度 /个300
250
200
150
100
50
0128 256 512 1 024 2 048 4 096
Bert-base Bert-large GPT- 2 GPT- 3
12