大语言模型算法演进综述 朱炫鹏  等 热点专题
中兴通讯技术
2024  年 4 月    第 30 卷第  2 期   Apr . 2024    Vol . 30  No. 2现象 ，如上下文学习 、思维链推理 。
1.2 大语言模型算法演进路线
大语言模型的发展主要有两条技术路线 ：一是提高推理
效率 ，降低推理成本 ；二是提高模型能力 ，迈向 AGI。
大语言模型能力强大 ，有广阔的应用前景 ，各厂商都在
积极部署 ，提供服务 。但是 ，由于模型规模巨大 ，算法对硬
件不够友好 ，需要消耗大量的算力 、存储 、能源 。因此 ，如
何降低推理成本 、推理延时 ，是一个亟待解决的问题 。大语
言模型主要的技术路线有分布式推理 、减小模型计算量 、减
小模型访存量 、提升硬件亲和性等 。
大语言模型是迈向 AGI的重大进步 ，而Transformer 是其
中的核心架构 ，发挥了重大作用 。但Transformer 也有一定的
不足 ，如计算量大 ，通过提升规模来提升性能更加困难 ；上
下文窗口长度有限 ，难以支持超长序列 。研究人员通过引入
新的结构 ，解决这些问题 ，取得了较好的效果 。2 大语言模型架构
2.1 Transformer
Transformer 模块是组成大语言模型的基础单元 ，由多头
注意力 、前馈网 、Softmax 、LayerNorm 等部分组成 ，本节中
我们主要介绍 Transformer 的结构和算法 。
2.1.1 注意力机制
注意力机制是针对一个文本序列 ，计算每个 token （符
号）与其他 tokens 之间的相关系数 ，找出相关度高的 tokens ，
用于生成特征 。例如 ， “这是一只猫 ，它很可爱 。 ”在这句话
里， “猫”与“这” “它”的相关度会比较高 。
注意力机制是基于查询 -键-值（QKV）计算的 。具体
算法为 ：输入Q、K、V，用Q和KT做矩阵乘 ，除以Dk，
做Softmax ，得到注意力矩阵 A；A和V做矩阵乘 ，得到输出
特征 。具体的公式如下 ，结构见图 1。
图1 Transformer 架构[3]矩阵乘
缩放层
矩阵乘连接
缩放点积注意力
线性层
KK
线性层
VV
线性层
QQ
注意力 多头注意力
输入 目标位置编码 位置编码
Transformer
编码器Transformer 
解码器输入嵌入向量 输出嵌入向量累加和归一化
前馈网
累加和归一化
多头自注意力归一化指数函数
线性层
累加和归一化
前馈网
累加和归一化
多头交叉注意力
累加和归一化
掩码多头
自注意力N xxN   输出概率
KK VVQQ KK VVQQKKVVQQ
QQ KK VV线性层
归一化
指数函数
10