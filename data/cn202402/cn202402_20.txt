大语言模型算法演进综述 朱炫鹏  等 热点专题
中兴通讯技术
2024  年 4 月    第 30 卷第  2 期   Apr . 2024    Vol . 30  No. 2GShard[51]和Switch Transformers[52]在大语言模型中加入
MoE，取得了较好效果 ，下面我们将详细介绍 。
4.1.1 GShard
文献 [51]认为 ，增加大语言模型的深度和宽度 ，计算复
杂度则会超线性增加 ，一旦超过了 O(n)，大规模训练难以
实现 。在Transformer 中加入 MoE，计算复杂度会明显下降到
小于O(n)，大规模训练则可以实现 。因此 ，文献 [51]提出了
GShard ，并在分布式训练 、推理中使用了专家并行 ，将多个
专家分布在不同的计算设备上 ，降低了每个设备的存储量 。
Transformer 中加入 MoE 的方法 ，将FFN 替换为 MoE。
MoE 中有多个专家 ，每个专家都是一个单独的 FFN。具体如
公式 （8）—（10） ：
Gs，E=GATE(xs)， （8）
FFNe(xs)=woe∙ReLU(wie∙xs)， （9）
ys=∑e=1EGs，e∙FFNe(xs)， （10）
其中 ，xs是MoE 的输入 ，向量Gs,E是门控网络 GATE 计算的
每个专家的选择概率 ，其元素为 Gs,e, e∈[1, E]，选择策略是
取概率最大的两个专家进行实际计算 ，其他专家的概率
设为 0。
MoE 中有E个专家 FFN1，…，FFNE，第e个专家内部的
计 算 为 输 入 全 连 接 层 （权 重 为wie） 、 线 性 整 流 函 数
（ReLU ） 、输出全连接层 （权重为woe） 。
ys是MoE 的输出 ，由选中的两个专家的计算结果与概率
相乘累加而成 。GShard 专家并行如图 5
所 示 。图5中 分 别 为 标 准
Transformer 编 码 器 、MoE 
Transformer 编码器和多设备
分布式的 MoE Transformer 编
码器 。MoE Transformer 编码
器与 Transformer 编码器的差
别 是 FFN 替 换 成 了 MoE，
MoE 放置在单计算设备上 ，
设备需要存储所有专家的权
重。多 设 备 分 布 式 的 MoE 
Transformer 编码器进一步实
现了专家并行 ，E个专家分
别放置在 E个计算设备上 ，
每个设备放置一个专家 ，因
此只需存储一个专家的权重 。在推理时 ，每个 token 通过门
网络选择两个专家 。如果专家就在自己所在的设备上 ，则直
接进行计算 ；否则 ，将token 通过 All-to-All 集合通信接口发
送到专家所在的设备 ，进行计算 ，结果再通过 All-to-All 集
合通信接口发回到原设备 。
4.1.2 Switch Transformers
Switch Transformers 也是将 FFN 替换为 MoE，不同的是
修改了门网络 ，每次只选择一个专家计算 ，称为 Switch 层。
这样有 3个好处 ：
• 减少了路由的计算量 ；
• 每个专家的  批量大小 （专家容量 ）至少可以减半 ；
• 路由执行简化 ，通信成本降低 。
虽然 Switch 层选择的专家数减小了 ，但模型的性能却比
普通 MoE 模型更高 。这说明在大语言模型场景 ，设备内存
相对稀缺 。如果专家接收过多的 tokens ，会因内存不足而丢
弃token ，这样会造成模型性能下降 。
Switch Transformers 使用数据并行 、模型并行 、专家并
行，在较大的集群上进行分布式训练 ，得到了参数量为
3.95×1011、1.571×1012的大语言模型 。
4.2 状态空间模型
Transformer 的优点是能够并行计算一个序列所有 tokens
间的相关信息 ；缺点是序列的长度有限 ，对于超过上下文窗
口长度的序列 （如音频 、视频 ）难以计算 。SSM 可以解决此
问题 。
SSM 是控制论里的概念 ，其作用是对一个输入连续时间图4 MoE 架构[51]MoE 层 MoE 层MoE 层
专家 1 专家 2 专家 3 专家 n-1专家 n ……G(x)2G(x)n-1
门网络
x
MoE ：混合专家
16