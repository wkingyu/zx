低资源集群中的大语言模型分布式推理技术 冯文佼   等 热点专题
中兴通讯技术
2024  年 4 月    第 30 卷第  2 期   Apr . 2024    Vol . 30  No. 2PyTorch[16]作为核心框架 。在计算方面 ，我们重新设计了分
布式推理的架构 ，并实现了精细的显存管理及调度策略 。在
通信层面上 ，我们依托 PyTorch-DDP ，打造了一种基于通信
树的高效 All-Reduce 集群通信机制 。我们在两台配置有双
Intel(R) Xeon (R) E5-2678  v3 CPU 、4块NVIDIA RTX 2080 TI 
GPU 、128 GB系统内存及 44 GB总显存的主机上开展实验 。
主机间通过 1 GB网络带宽互联 。实验采用 Meta AI 发布的
LLaMA- 3B。表2展示了默认的超参数配置 。
3.2 延迟和吞吐量
我 们 选 择 基 于 分 布 式 数 据 并 行 （DDP）的 原 生 All-
Reduce 作 为 Benchmark ，采 用 参 数 服 务 器 （PS）架 构 。
worker 节点通过采用 “星形 ”拓扑结构进行通信 ，即多个
worker 直接与中心服务器进行数据交换 。
我们首先对 Benchmark 和本方案在首词元生成延迟及每
秒生成词元吞吐量方面进行了比较测试 。其中 ，首词元生成
延迟涵盖模型处理输入并自回归生成下一词元的计算及通信
延迟 ，每秒生成词元吞吐量用每秒可以处理的词元数来衡
量。如图 4所示 ，我们测试了不同的输入词元数 。相比于Benchmark ，本方案的首词元生成延迟降低 34%~61%，每秒
生成词元吞吐量提升 52%~150%。这证明了上述技术的有
效性 。
3.3 显存占用
为评估本方案的显存管理与调度效能 ，我们比较了启用
与未启用本显存管理方案时 ，首词元生成阶段节点的峰值显
存占用情况 。图5展示当输入词元数量增加时节点显存占用
的线性增长趋势 。与未启用显存管理相比 ，本方案的显存占
用降低 61%。这也验证了 2.3节中的分析 。
4 结束语
在面对全球竞争和资源限制的挑战下 ，我们提出了一种
适应弱算力及小显存环境的分布式 LLM 推理架构 。同时通
▼表2 LLaMA- 3B模型默认超参数设置
变量名
注意力机制中的头数
批量大小
隐藏层的维度大小
模型中的层数
序列长度
词汇表的大小符号
Natten_heads
Bsize
Hmodel
Nlayers
seq_len
vocab_size值
32
32
3 200
26
2 048
32 000
图4 不同方案下延迟和吞吐量对比图5 显存管理对节点显存占用的影响
（a） 首词元生成延迟 （b） 每秒生成词元吞吐量延迟/s
10 20 30 40 50 10 20 30 40 50
输入词元数 输入词元数吞吐量 （生成词元数 /s）Benchmark
本方案Benchmark
本方案 120
100
80
60
40
20
030
25
20
15
10
5
010 20 30 40 50 60
输入词元数显存占用 /GB显存管理策略性能分析
启用显存管理 未启用显存管理8
6
4
2
0
48