大模型关键技术与应用 韩炳涛  等 企业视界
中兴通讯技术
2024  年 4 月    第 30 卷第  2 期   Apr . 2024    Vol . 30  No. 2特点 ，算子融合主要分为 4类：归一化层和 QKV 横向融合 ，
自注意力计算融合 ，残差连接 、归一化层 、全连接层和激活
层融合 ，偏置加法和残差连接融合 。
中兴通讯在 vLLM （开源项目名 ）上实现了针对多查询
注意力结构的 QKV 通用矩阵乘法 （GEMM ）横向算子融合 ，
以及多层感知机 （MLP）中的全连接层 +激活融合 ，性能明
显提升 ，见表 1和表 2。上述算法的相关代码实现已合入
vLLM 社区 。
由于算子融合一般需要定制化来实现算子 CUDA kernel ，
因此对 GPU 编程能力要求较高 。随着 TensorRT 、OpenAI 
Triton 、张量虚拟机 （TVM）等框架引入编译器技术 ，算子
融合的自动化或半自动化逐步实现 ，这降低了 GPU 编程难
度，取得了较好的效果 。
3.3 模型压缩
模型压缩技术是指在不影响模型精度的情况下 ，通过缩
小模型规模和计算量来提高模型的运行效率 。常见的深度学
习模型压缩技术包括模型剪枝 、知识蒸馏 、模型量化和模型
分割等 。其中 ，模型量化在这些技术中最具实用性 。
SmoothQuant[39]是典型的 8 bit LLM 量化方法 。根据激活
量化方式的不同 ，SmoothQuant 提供了 3种量化方式 ：per-
tensor static 、per-tensor dynamic 和per-token dynamic 。这3种
模型的精度依次提升 ，计算效率依次降低 。SmoothQuant 的
研究人员观察到 ，不同的标记在它们的通道上展示出类似的
变化 ，引入了逐通道缩放变换 ，有效地平滑了幅度 ，这使得
模型更易于量化 。激活感知权权重化 （AWQ ）[38]和生成式
预训练 Transformer （GPTQ ）[37]是典型的仅权重量化的方法 ，
且权重量化的是 group 粒度 。GPTQ 提出了一种基于近似二阶信息的新型分层量化技术 ，使得每个权重的比特宽度减少到
3或4位。与未压缩版本相比 ，该技术几乎没有准确性损失 。
AWQ[18]的研究人员发现 ，对于 LLM 的性能 ，权重并不是同
等重要的 ，仅保护 1%的显著权重可以大大减少量化误差 。
在此基础上 ，AWQ 采用了激活感知方法 ，这在处理重要特
征时起着关键作用 。该方法采用逐通道缩放技术来确定最佳
缩放因子 ，从而在量化所有权重的同时最小化量化误差 。
中兴通讯提出了 SmoothQuant+[40] 4 bit权重量化训练后量
化（PTQ）算法 。不同于 AWQ 对单个层搜索量化参数 ，
SmoothQuant+ 对整个模型搜索量化参数 ，并对整个模型进行
同一个参数平滑激活 ，这样能够从模型整体减少量化误差 ，
且搜索效率更高 。SmoothQuant+ 在LLaMA 系列模型可以得到
比AWQ 更好的精度 （见表 3） ，同时在性能上也优于 AWQ ，
对应的推理核已开源 。
随着大模型上下文长度的增加 ，KV Cache 占用的显存
将超过权重和激活 ，因此对 KV Cache 进行量化可以显著降
低大模型在长上下文推理时资源占用 ，从而允许系统支撑更
多的并发请求数和吞吐率 。中兴通讯在生产环境中使用
INT4权重量化和 KV Cache FP 8量化 ，显存节省了 70%，吞
吐率提升了 2.8倍，推理成本降低 75%左右 。
3.4 并行推理
当大模型参数量超过单一计算设备所能容纳的上限时 ，
则需要使用分布式并行推理技术 。并行推理可以使用模型并
行和流水线并行 ，而模型并行由于可节省显存资源 、可降低
单用户时延等优势 ，成为首选的并行方式 。
业界最流行的模型并行方案来自 Megatron-LM[21]，它的
开发者针对 Self-Attention 和MLP 分别设计了简洁高效的模
型并行方案 。MLP 的第 1个全连接层为 Column Parallel ，第2
个全连接层为 Row Parallel ，之后是 1次AllReduce 规约操作 。
Self-Attention 在计算 Query 、Key和Value 向量时执行 Column 
Parallel （按注意力头个数均分到每个 GPU） ，之后将注意力
得分做空间映射时执行 Row Parallel ，之后是 1次AllReduce
规约操作 。除此之外 ，LLM 模型中的 Input Embedding 采用▼表1 StarCoder- 15B在A100-40GB上测试查询 -键-值融合
批大小 /
样本数
10
30输入长度 / 
token 数
1 024
1 024输出长度 / 
token 数
1 024
1 024注意力
基线/s
27.17
39.08注意力
融合/s
22.80
37.48Speedup/
%
19
4
▼表2 StarCoder- 15B在A100-40GB上测试 FC+激活融合
实测的  
TFLOPs
基线
融合 MLP
加速率B=1，
M=1，
K=6 144
0.3
0.3
0.0%B=4，
M=1，
K=6 144
1.2
1.2
0.0%B=16，
M=1，
K=6 144
4.7
4.9
4.3%B=64，
M=1，
K=6 144
17.6
19.1
8.5%B=256，
M=1，
K=6 144
30.3
47.1
55.4%
FC：全连接层           MLP ：多层感知机
TFLOPs ：每秒钟浮点计算万亿次数
注：B代表 Batchsize ；M和K表示矩阵乘法中的两个维度 ，M恒为 1（解码阶
段） ，K恒为 6 144▼表3 CodeLLaMA INT 4量化在 HumanEval 上的性能
HumanEval ↑
FP16（baseline ）
RTN
AWQ
SmoothQuant+7B/%
35.98
36.59
35.98
3535..989813B/%
35.98
33.54
31.71
3737..808034B/%
51.22
46.34
50.61
5353..0505
AWQ ：激活感知权权重化
FP16：16 bit原始精度RTN：直接量化
82