大语言模型算法演进综述 朱炫鹏  等 热点专题
中兴通讯技术
2024  年 4 月    第 30 卷第  2 期   Apr . 2024    Vol . 30  No. 2暂停 Flash-Decoding ，回退到原始算法 。
3.2.3.3 PagedAttention
KV Cache 显著减小了模型的计算量 ，但也存在一些缺
点：一是显存占用大 ，达到数 GB以上 ；二是大小动态变化 ，
随着序列长度的不同 ，大小可相差数千倍 ，且不可预测 。这
给有效管理 KV Cache 带来了很大的挑战 。研究发现 ，由于
碎片化和过度保留 ，现有系统浪费了 60%～80%的显存 。受
操作系统中虚拟内存和分页经典思想启发 ，PagedAttention[34]
允许在非连续的内存空间中存储连续的 KV Cache ，有效提
高内存的利用率 。同时 PagedAttention 带来另一个关键优势 ：
高效的内存共享 。例如 ，在并行采样中 ，多个输出序列是由
同一个提示生成的 ，提示的 KV Cache 页面可以在输出序列
中共享 。
3.2.4 量化
量化是深度学习模型通用的压缩方法 ，将模型的权重 /
激活数据格式转换为 INT8、INT4等整数 ，以降低计算量和
存储量 。大语言模型量化按阶段分有 3种：训练感知量化
（QAT） 、训练后量化 （PTQ） 、微调感知量化 （QAFT ）[35]。
由 于 大 语 言 模 型 训 练 困 难 ，因 此 QAT 用 得 较 少 ，PTQ、
QAFT 用得比较多 ，本文中我们主要讨论 PTQ。
3.2.4.1 仅量化权重
大语言模型训练完成后 ，权重是已知的 ，而激活则要等
到推理时才能知道 ，且取值范围与输入的序列相关 。因此 ，
权重比较容易量化 ，误差较小 ；而激活的量化较难 ，如果遇
到离群值 ，误差会比较大 。基于这样的特点 ，我们可以对模
型 做 混 合 精 度 量 化 ，方 法 有 LLM .int8()[36]、GPT 量 化
（GPTQ ）[37]、不相干处理量化 （QuIP ）[38]、激活感知的权重
量化 （AWQ ）[39]、离群值感知的权重量化 （OWQ ）[40]、稀
疏量化表示 （SpQR ）[41]、细粒度权重量化 （FineQuant ）[42]。
LLM .int8()认为 ，激活中的离群值很重要 ，因此在计算
矩阵乘法时 ，需要对离群值和正常值做不同的处理 ：
• 取出激活中异常值所在的列以及权重中对应的行 ，保
持FP16格式 ，计算点乘 ；
• 剩下的激活正常值 ，与对应的权重量化到 INT8，计算
点乘 ，再反量化成 FP16；
• 两者的结果累加 ，得最终结果 。
3.2.4.2 权重和激活都量化
ZeroQuant[43]对权重做按组量化 ，对激活做按 token 量化 ，并逐层使用知识蒸馏缓解精度损失 。此方法在 BERT 和
GPT- 3模型上精度较高 。
针对激活的离群值 ，SmoothQuant[44]的量化方案是 ：激
活有离群值 ，权重无离群值 ，如果把两者平均一下 ，让它们
的数值都落入正常范围 ，就容易量化了 。具体做法是 ，按通
道统计激活的取值范围 ，如果发现有离群值 ，则把该通道的
数据都除以系数 a，权重中对应通道数据都乘以 a，这样最
终的计算结果不变 。
类似的方法还有 OliVe[45]、基于重排列的训练后量化
（RPTQ ）[46]、量 化 大 语 言 模 型 （QLLM ）[47]、Outlier Sup ‐
pression+[48]。
4 大语言模型的性能提升
将大语言模型加入新的结构 ，也可以提升模型性能 。典
型的结构有 MoE 和状态空间模型 （SSM）[49]。
MoE 采用稀疏化策略 ，即在模型中放置大量专家 ，每个
专家代表一个领域 ，推理时每次只启用少量专家 。这样的
话，模型可以在整体上关注更多的领域 ，而推理时 ，单个
token 只关注特定领域 ，避免了在无关领域上浪费算力 。这
样既减小了计算量 ，又提高了模型性能 。
SSM 不受上下文窗口长度的限制 ，能够处理超长序列 ，
选择性记录上下文信息 ，可以在音频 、文本等方面展现出良
好的性能 。SSM 在结构上综合了 CNN 与RNN 的特点 ，计算
量、存储量比 Transformer 更小 。
4.1 MoE
文献 [16]首先提出了 MoE 的概念 ，认为用单个模型去适
应多个场景的样本 ，会受到很多干扰 ，因此会导致学习很
慢、泛化困难 。使用多个模型 ，且每个模型学习一个场景 ，
就可以得到比较好的效果 。在多个模型之前增加一个门网
络，就可以决定每个数据应该被哪个模型处理 。
文献 [50]提出将 MoE 引入语言模型 ，并做了两点创新 ：
• Sparsely-Gated ：门网络每次只选择两个专家进行计
算，显著降低了计算量 ；
• token-level ：对一个序列中的每个 token ，各自独立地
分别选择专家 ；
文献 [51]的MoE 架构如图 4，推理的计算步骤如下 ：
• 输入x进入门网络 ，经过一个全连接层 ，计算出每个
专家的概率 ；
• 选出概率最高的两个专家 ，将x输入到这两个专家中 ；
• 两个专家计算的结果 ，与前面的概率相乘累加 ，得最
终输出 。
15