大模型关键技术与应用 韩炳涛  等 企业视界
中兴通讯技术
2024  年 4 月    第 30 卷第  2 期   Apr . 2024    Vol . 30  No. 2（LSTM ）[2]等循环神经网络计算效率低 、训练容易过拟合的
问题 。此后 ，OpenAI 和Google 在Transformer 基础上 ，又分
别提出了 GPT[3]和BERT[4]。GPT 采用了 Transformer 的解码器
部分 ，使用从前到后的单向预测模式 （类似于补全 ） 。BERT
则采用了 Transformer 的编码器部分 ，使用上文与下文的双向
预测模式 （类似于填空 ） 。受益于该模式 ，BERT 实现了较
强的性能 ，让业界一度认为双向语言模型是更优的选择 。
但是 ，OpenAI 笃定追逐 “通用语言模型 ” ，认为从前到
后的生成能力可以转化应用于各类语言任务上 ，因此在后续
模型中依然坚持单向预测模式 。直到 2020 年，OpenAI 提出
了拥有 1 750亿参数的 GTP- 3[5]模型 。GTP- 3在对话 、知识问
答、吟诗作赋等多项任务中展示出的能力均令人印象深刻 。
此后 ，OpenAI 不再公开模型相关的技术细节 ，研究人
员开始把目光聚焦在其他开源模型上 。2023 年Meta 发布
LLaMA 系列模型[6]，进一步优化了 Transfomer 模型架构 ，并
使用更加充分的数据对模型进行训练 ，获得了不错的性能 。
此后 ，许多研究人员相继基于 LLaMA 模型不断地做局部的
优化 ，如Baichuan 、Yi、Mistral 、Qwen 等。
1.2 对计算效率的优化
相较于之前 LSTM 等循环神经网络 ，Transformer 最大的
弱点是计算效率低 ，主要原因是其自注意力机制的计算复杂
度与序列长度的平方成正比关系 。针对该问题 ，一条技术路
线是放弃 Transformer ，设计更高效的模型结构 。目前我们认
为 有 可 能 取 代 Transformer 的 架 构 主 要 包 括 Linear Trans ‐
former 、RWKV 和Mamba 等。Memba 采用了完全不同的模型
架构[7]，彻底抛弃了注意力机制 ，从状态空间的角度对序列
进行建模 。相关研究表明 ，该模型把训练计算复杂度降到线
性，且能力与 Transformer 相当 。A21 Labs 刚刚发布了首个生
产级别的基于 Mamba 的模型 Jamba ，模型参数达到了 520亿，
同时提供长达 256 k的上下文 。尽管当前 Memba 在参数规模
上与 Transformer 仍然有较大差距 ，但前景仍被看好 。业界认
为Memba 是取代 Transformer 的有力竞争者 。
另一条技术路线是继续优化 Transformer 模型结构 。例
如，在注意力机制方面 ，代表性的工作包括将多头注意力机
制（MHA ）改进为多查询注意力机制 （MQA ）和分组查询
注意力机制 （GQA） ， 这可以进一步降低计算量 。其中 ，
LLaMA 2采用了分组查询注意力机制[8]，Google 最近发布的
Gemini 采用了多查询注意力机制 。
除了结构上的改进 ，优化算子实现也可以提高效率 ，例
如：FlashAttention 通过 Attention 与Softmax 计算融合 ，结合
KV Cahche ，能显著提高计算速度并降低显存带宽依赖[9]，从而使 Transformer 的计算效率接近线性注意力模型 。
1.3 长上下文推理
能够处理的上下文序列长度是语言模型能力的一个关键
指标 。在许多任务中 ，如文章阅读理解 、代码生成 、检索增
强生成 （RAG）等，都需要模型能够处理很长的上下文 。然
而，长上下文模型训练与推理 ，存在计算复杂度高 、最大长
度约束等问题 。
最大长度约束问题 ，是指模型在实际推理时处理的序列
长度超过训练时序列长度 ，这可能会导致性能的明显下降 。
针对该问题 ，有两种解决方案 ：一种是预训练过程中调整模
型设计 ，可以实现更好的模型外推能力 ；一种是通过微调和
位置嵌入处理 ，扩大模型的上下文窗口 。例如 ，苏剑林发现
如果对注意力矩阵的查询 -键-值（QKV）映射矩阵加入
Bias，则使用旋转位置编码 （ROPE ）的模型可能会获得较
好的外推能力 。该方法常应用在 Qwen 模型中[10]。此外 ，研
究人员发现 ，如果对 ROPE 进行插值 ，并配合简单的微调 ，
可以把上下文窗口的大小从 4 096扩展到 32 000[11]。
研究人员通过对改进注意力机制 ，可以降低计算复杂
度，其中最具代表性的是 Window Attention 方法 。该方法引
入了注意力窗口 ，让每层注意力仅关注序列的局部信息而非
全局信息 ，通过层层堆叠放大模型的上下文感受野 。这样就
可以把计算复杂度控制在一个明确的范围内 ，避免随序列长
度的无限制增长 。该方法常应用在 Mistral 中[12]。此外 ，还有
其他一些替代注意力机制的方法 ，如上文介绍的线性注意力
机制与 Mamba 等模型等 。
1.4 多模态能力
多模态大模型可以分为多模态理解大模型和多模态生成
大模型 。多模态理解大模型输入多模态信息 ，输出中包含文
本信息 ；多模态生成大模型则相反 ，其输入中包含文本信
息，输出为多模态信息 。
自从经典的 BLIP 2模型出现后 ，多模态大模型设计趋势
逐渐稳定 ，具体如图 1所示 。每种模态均有对应的编码器和
图1 多模态大模型结构示意图文本
图像
声音
视频编码器
编码器
编码器
编码器解码器
解码器
解码器
解码器跨模态对齐
77