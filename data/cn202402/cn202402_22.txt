大语言模型算法演进综述 朱炫鹏  等 热点专题
中兴通讯技术
2024  年 4 月    第 30 卷第  2 期   Apr . 2024    Vol . 30  No. 2Kˉ=(CBˉ，C------AB，...，CAˉkBˉ)，
y=x*Kˉ。 （16）
因此 ，S4模型在训练时可以使用卷积形式计算 SSM，在
推理时可以使用递归形式计算 SSM。
4.2.2 Mamba 模型
Mamba 在S4的基础上做了 3点改进 ：
1）对输入信息有选择的处理
序列建模的一个基本问题是把上下文压缩成更小的状
态。从这个角度来看 ，注意力机制虽然效果好 ，但效率不是
很高 。因为它不压缩上下文 ，而是全部存储 （也就是 KV 
Cache ） ，这直接导致训练和推理消耗算力大 。RNN 压缩全
部上下文 ，但压缩率随着序列长度增加而增大 ，最终会丢失
长期信息 ，因此推理和训练效率高 ，但性能较差 。Mamba 的
解决办法是 ，让模型对上下文有选择的处理 ，丢弃不重要信
息，对重要信息进行压缩保留 ，在计算效率和保留信息两方
面取得平衡[59]。
S4的参数矩阵 B、C和步长Δ，是所有 tokens 共享的 ，
每个 token 计算使用相同的一套 B、C、Δ。Mamba 的改进点
是，用输入序列 x（长度为L）经过 3个全连接层 ，生成L套
不同的B、C、Δ，每个 token 计算使用一套 ，从而实现对信
息的选择性处理 ，关注或忽略特定的内容 。
2）硬件感知算法
Mamba 在GPU 上对 SSM 算法做了优化 ，利用扫描而不
是卷积来递归计算模型 ，尽量减少不同级别的 GPU 内存层
次结构之间的 IO访问 。3）更简单的架构
Mamba 架构是 SSM 与Transformer 的多层感知机 （MLP）
块的结合 ，两者相互融合 ，而不是重叠 ，形成了一个更加简
单的结构 ，如图 6所示 。
Mamba 在合成 、音频 、基因 、语言等多个领域的任务上
都表现出良好的性能 ，并能处理超过 1 M长度的序列 。
5 结束语
大语言模型是神经网络模型长期发展的成果 ，特别是
Transformer 结构的注意力机制与大算力结合 ，产生了涌现能
力，其文本理解 、生成 、对话能力已接近人类 。但Trans ‐
former 的一些固有特点也导致了其性能的不足 ，如生成式模
式、自注意力造成消耗算力过大 ，上下文长度有限 ，硬件亲
和性差造成推理性能低等 。目前 ，大语言模型算法的演进是
渐进的优化 ，对Transformer 结构做局部的修改 ，以改善这些
不足 。未来的演进可能仍然是渐进的 ，也可能会产生革命性
的创新 ，即提出全新的模式 ，代替生成式模式和自注意力 ，
增强逻辑推理能力和世界模型认知 ，更加接近 AGI。
参考文献
[1] OpenAI . Introducing ChatGPT [EB/OL ]. [2024 -03-10]. https ://
openai .com/blog/chatgpt/
[2] OpenAI . GPT- 4 technical report [EB/OL ]. [2024 -03-10]. DOI : 
10.48550 /ARXIV .2303 .08774
[3] VASWANI A , SHAZEER N , PARMAR N , et al . Attention is all you 
need [EB/OL ]. (2017 -06-12) [2023 -08-02]. https ://arxiv .org/abs/
1706 .03762
[4] 车万翔 , 刘挺 .自然语言处理新范式 ： 基于预训练模型的方法  [J]. 中
兴通讯技术 , 2022 , 27(2): 3-9. DOI: 10.12142 /ZTETJ .202202002
图6 Mamba 架构[49]SSM
Conv
H3 门控 MLP Mamba
MLP ：多层感知机      SSM ：状态空间模型ConvSSM线性投影
序列变换
非线性
（激活或相乘 ）σ σ σ
18