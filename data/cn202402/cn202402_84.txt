大模型关键技术与应用 韩炳涛  等 企业视界
中兴通讯技术
2024  年 4 月    第 30 卷第  2 期   Apr . 2024    Vol . 30  No. 2键技术 ——精调训练 ，即针对特定任务或应用场景 ，在预训
练模型的部分或全部参数上进行进一步的学习与优化 ，提升
模型在特定任务上的遵循指令能力 、问题解决能力 、特定表
达方式能力等 ，从而提高其在该任务上的精度和专业性 。
精调训练主要分为全量精调和低资源精调两大类别 。其
中，低资源精调主要指的是 LoRA[24]、prefix-tuning 以及 P-
tuning 等方法 。这些方法对模型局部进行微调或者冻结一部
分参数进行微调 。其中 ，LoRA 方法效果最佳 。该方法通过
低秩近似对预训练模型的部分权重矩阵进行更新 ，在降低存
储成本和计算复杂度的同时 ，实现模型对目标任务的快速适
应。全量精调则在训练过程中会对整个模型的参数进行优化
和更新 ，不仅对计算资源和存储资源要求更高 ，也更容易出
现过拟合 、丧失通用性等问题 。但全量精调拥有更高的上
限，通过适当的训练优化来增强模型的泛化性 。全量精调方
法能够得到比低资源精调方法更优秀的性能 。
由于 “对齐税 ”问题 ，精调训练在提升特定任务表现的
同时会影响模型在其他任务中的总体表现 。因此 ，使用 “少
而精 ”的精调训练数据 ，运用 Dropout 等技术可以防止过拟
合，对平衡大模型的泛化能力和任务适应性尤为重要 。
2.4 对齐训练
大语言模型的一些不良行为 （例如 ，不真实的回答 、谄
媚和欺骗 ）激发了业界对人工智能对齐领域的深入研究 。AI
对齐旨在使人工智能系统的行为与人类的意图和价值观相一
致，在通往 AGI的道路上 ，AI对齐无疑是安全打开 “潘多拉
魔盒 ”的密钥 。
对齐训练主要应用强化学习算法 ，其基本方法是首先基
于人类标注数据训练评分模型 ，然后再基于评分模型运行强
化学习算法 ，通过引导模型获得更高的评分 ，使其输出更符
合人类标准 。OpenAI 首先提出的基于人类反馈的强化学习
（RLHF ）就是利用人类的反馈来优化模型的输出 ，使其更符
合人类的偏好和价值观 。这不仅能够提升语言模型性能 ，也
能提升安全性和有用性 。相比于 RLHF 涉及多个模型和不同
训练阶段的复杂过程 ，拒绝采样 （RS）[25]则更为简洁有效 。
RS是指让一个模型针对同一个 prompt 生成 K个不同答案 ，
然后利用奖励模型为这 K个答案打分 ，选出最优的答案后 ，
再用最优问答样本对原模型进行监督微调 ，以增强模型能
力。直接偏好优化 （DPO）[26]也是 RLHF 的替代方案之一 。
DPO 利用奖励函数和最优策略之间的映射关系 ，将约束奖励
最大化问题转换为单阶段的策略优化问题 。DPO 算法因无须
拟合奖励模型 ，且无须在微调期间从 LM采样或执行重要的
超参数调整 ，从而实现了稳定性高 、性能强且计算量轻等优秀表现 ，大大简化了实施和训练过程 。
中兴通讯的星云大模型在 RLHF 这一框架的基础上 ，通
过设计质量评优模块并作为对样本自动打分的奖励模型 ，同
时结合 RS拒绝采样选取最优样本 ，经过多次迭代生成更多
的高质量代码数据 。这使得大模型从这些高质量数据中不断
训练优化 ，从而提升了生成代码的质量 。星云大模型的
HumanEval@Pass 1可以达到 83.6分。
2.5 合成数据和自我学习
随着模型规模不断增大 ，所需的训练数据也更多 ，最终
将会耗尽所有自然产生的数据 。因此 ，基于人工合成数据来
训练模型已经成为一个热门研究方向 。
借助已有大模型合成精调数据是业界最常用的方式 ，通
过将设计好的指令 ，并让大模型来获取对应的回答 ，可以节
省大量的人力 。WizardLM[27]和WizardCoder[28]根据种子指令
分别沿着添加约束 、深化 、具体化 、增加推理步骤 、复杂化
以及突变等 6种演化方向来演化指令 ，并控制指令的难度和
复杂程度 ，经过多轮不同方向的演化得到大量不同类型的指
令，再利用已有大模型生成精调数据 ，最终取得了不错的精
调效果 。Magicoder[29]通过在 GitHub 上随机获取 1~15行代码
作为种子代码片段 ，让已有大模型根据提供的种子片段来生
成编程问题 ，这大大丰富了代码精调数据的类型 ，从而更好
地激活大模型的能力 。
中兴通讯的星云大模型采用自我学习的方法生成测试用
例数据 ，主要的步骤是通过对同一代码多次生成对应的测试
用例 ，根据编译 、覆盖率等情况选出符合要求的高质量数
据，再使用这部分高质量数据对模型进行精调训练 ，从而取
得良好效果 。在基于 HumanEval 数据集制作的 UTEval 数据
集上 ，星云大模型测试用例生成的编译成功率 、用例通过
率、测试覆盖率指标已经超过 GPT4-turbo 。
2.6 模型安全性
大模型在提供强大的自然语言处理能力的同时 ，也带来
了安全和隐私方面的挑战 。在训练阶段 ，大模型面临的风险
主要源于训练数据 。数据可能包含有害内容或设计歧视 、侵
权、毒性文本 。这不仅会影响模型的输出质量 ，还可能使模
型学习并复制这些不当行为 。此外 ，训练数据还可能遭到恶
意投毒 ，即意图故意降低模型的性能或引导模型做出不当
行为 。
训练阶段引入的常见漏洞包括后门漏洞和数据投毒 。针
对以上漏洞 ，可以实施训练语料库治理手段 ，具体包括 ：
1）语言识别和解毒 ：通过自动化工具识别并清除训练
80