NetGPT ：超越个性化生成服务的内生智能网络架构 陈宇轩  等 热点专题
中兴通讯技术
2023  年 10 月    第 29 卷第  5 期   Oct . 2023    Vol . 29  No. 5如图 1所示 ，有几种方法可以实现 LLM 的云边协同部
署，如本地微调 、模型拆分等 。具体而言 ，本地边缘服务器
可通过卸载云端训练的 LLM 来定制 LLM ，实现个性化 、定
制化服务 ，满足用户喜好及场景需求 。在此场景下 ，联邦学
习或并行训练可作为一种辅助手段来实现调优[2，4]。但对完
整LLM 的重复微调意味着庞大的计算 ，并有可能会给模型
开发人员带来知识产权上的困扰 ，因此该方法在实际应用中
也存在着诸多问题 。同时 ，对边缘处的 LLM 整体进行强制
拟合可能会使边缘服务器受限 ，计算资源紧张 ，从而导致边
缘计算所需开销过于庞大 。另外 ，卸载 LLM 也会产生明显
的通信开销 。另一种方案是将 LLM 拆分后部署到云端和边
缘 服 务 器 ，通 过 在 边 缘 部 署 一 些 大 规 模 深 度 神 经 网 络
（DNN）层，将剩余的层留给云端 ，从而有效平衡边缘服务
器和云端服务器间的计算资源 。在模型划分中 ，如何有效地
将DNNs 从边缘到云端进行划分是最具挑战性的问题之一 ，
这需要在最小化端到端时延的同时 ，为边缘服务器保留足够
小的模型尺寸[4]。考虑到典型的 LLM 中有数十亿个参数 ，这
样的模型划分可能会非常复杂 ，LLM 中广泛采用的参差链接
也可能会限制合适划分点的选择 。另外 ，LLM 可能会泄露训
练数据中的隐私细节[5]，因此直接以局部微调与模型分割的
方法来实现云边协同 ，也会存在一定的挑战 。
本文中 ，我们提出内生智能网络架构 NetGPT ，基于云
边不均衡的资源分布 ，实现了边缘与云端之间不同尺寸功能
性LLM 的协同 。与具有解耦 C&C 资源的 AI外生网络明显不
同，NetGPT 能够使用融合 C&C 对边缘部署更小的 LLM ，对
云端部署更大的 LLM ，以进行有目的的云边协同计算 ，从而提供个性化的内容生成服务 。此外 ，NetGPT 还集成并发展
了有逻辑的 AI工作流 ，以识别具有相同性能的通信链路 。
例如 ，在NetGPT 中，假设边缘 LLM 提供满意的内容 ，那么
性能驱动的通信链路会在边缘终止以加速响应 。否则 ，在即
时学习[6]理念的影响下 ，边缘 LLM 能够推断上下文并主动附
加（或填充 ）部分个性化信息 ，从而在云端得到更加全面的
效果 。同时 ，边缘 LLM 有助于为智能网络管理和调度 （例
如用户意图推断 、流行度预测等 ）提供统一的解决方案 。因
此，NetGPT 符合 C&C 深度融合的发展趋势 ，代表了一种由
LLM 驱动的内生智能网络架构 。
1 NetGPT 的实现
我们提出了一个云边协同框架 ，具体如图 2所示 。通过
在云端和边缘 （如基站 ）使用不同的预训练 LLM ，可以完成
个性化的生成服务 。受限于开源 LLM 的可用性 ，我们选择
并 部 署 了 LLaMA- 7B模 型 （Meta AI 大 语 言 模 型 -70亿 版
本）[7]和基于 GPT- 2的模型 。这两个模型分别由大约 67亿个
和1亿个参数组成 ，并部署在云端和边缘 。值得注意的是 ，
NetGPT 可以根据需要使用其他 LLM 。基于此 ，我们对云边
LLM 协同 NetGPT 的实现细节进行了深入研究 。首先 ，我们
对作为 LLM 基础的 Transformer 进行了总体概述 ，并列出了
LLaMA- 7B模型和 GPT2基础模型两个 LLM 的详细 DNN 结
构。随后 ，我们探讨了在计算受限的设备上微调 LLM 的有
效途径 ，并且展示了其对基于位置的个性化生成服务的
效果 。
图1 NetGPT 云边协同的方法LLM：大型语言模型云端 云端 云端云端
边缘
计算面
用户面
控制面
终端卸载&微调 拆分 协同
边缘 边缘 边缘 边缘 边缘 边缘
（a）LLM 卸载微调 （b）LLM 拆分 （c）LLM 协同
69