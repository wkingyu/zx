NetGPT ：超越个性化生成服务的内生智能网络架构 陈宇轩  等 热点专题
中兴通讯技术
2023  年 10 月    第 29 卷第  5 期   Oct . 2023    Vol . 29  No. 51.1 Transformer 概述
Transformer 已被广泛用作 LLM 中多层解码器的基础模
型。Transformer 是通过使用多层自注意力和前馈神经网络
（FNN）来构建 DNN 结构 。自注意力依赖于由 query 、key和
value 矩阵 （即QQ、KK和VV）定义的参数化注意力头 ，通过推
导不同的权重并将其分配给序列中的不同位置来计算输入序
列内的内部相关性 。在FNN 中，每个位置的表示使用的是
非线性变换 。此外 ，Transformer 采用层归一化等技术来缓解
梯度消失的问题 。
1.2 边缘和云端 LLM的DNN结构
1.2.1 GPT- 2-base 模型的 DNN 结构
GPT- 2-base 模型是 GPT- 2系列中最小参数的版本 ，包
含了原 Transformer 结构的 12层堆叠 （即8头自注意力子层和
FNN 子层 ） 。该模型利用正余弦位置的固定绝对位置编码方
式来预变换输入序列 。此外 ，GPT- 2使用修正线性单元
（ReLU ）来激活函数 。该模型具有相对优异的性能和较低的
计算要求 ，因此适合部署在网络边缘 。1.2.2 LLaMA 模型的 DNN 结构
LLaMA 经过大量无标签数据的训练 ，非常适合下游任
务 的 微 调 ，同 时 也 有 多 种 参 数 版 本[7]。与GPT- 3相 比 ，
LLaMA 结合了多项特定增强功能 ，从而在保持相似性能的
同时显著减少了参数数量[7]。为了提高训练稳定性 ，LLaMA
采用了对各子层的输入进行归一化而非对输出进行归一化的
方式 。此外 ，LLaMA 使用一种简化的替代方案 ，即采用均
方根层归一化 （RMSNorm ）函数[8]，利用均方根而非标准差
进行归一化处理 。此外 ，RMSNorm 引入了可学习的缩放因
子进行自适应特征缩放 ，从而增强具有不同值域的各种特征
的 归 一 化 效 果 。其 次 ，LLaMA 用Swish-Gated 线 性 单 元
（SwiGLU ）[9]取代了 ReLU 激活函数 。该激活函数将 Swish 函
数（即fSwish(x)=x∙σ(βx)，其中σ(x)= 1
1+e-x，β为可训练
参数 ）和门控线性单元 （GLU） （即fGLU(x)=x∙σ(Wx+b)，
W和b为可训练参数 ）相结合 ，从而可以根据输入以更有选
择性的方式激活神经元 ，在输入发生改变时更加平滑 ，以有
效捕获复杂的非线性关系 。最后 ，LLaMA 引入了旋转位置
嵌入 （RoPE ）[10]，利用预设的旋转矩阵对位置信息进行编
图2 面向 NetGPT 的云边协同计算框架GPT：生成式预训练
LLaMA- 7B：Meta AI 大语言模型 -70亿版本LoRA ：低秩适应算法
RMSNorm ：均方根归一化RoPE ：旋转位置嵌入
Softmax ：归一化指数函数SwiGLU ：Swish 门控线性单元提出目标 添加本地信息并扩展为完整提示 根据网络边缘提供的提示提供答案响应式 LLaMA- 7B综合提示 改进 Transformer 模块模块输入
模块输入简明提示微调的
GPT- 2-base……终端 边缘 云端
输出
综合提示
层归一化
前馈神经网络
层归一化
掩蔽自注意力
输入嵌入RMSNormQQ KK VVRoPE RoPECasua 1
maskSoftmax线性层RMSNorm
线性层
线性层
线性层线性层
RMSNorm
SwiGLU
输入嵌入丢弃单元Softmax
……改进 Transformer
模块
改进 Transformer
模块LoRAr
r
dindout
12×
70