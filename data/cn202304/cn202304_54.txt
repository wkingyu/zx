大规模语言模型的跨云联合训练关键技术 潘囿丞  等 热点专题
中兴通讯技术
2023  年 8 月    第 29 卷第  4 期   Aug . 2023    Vol . 29  No. 4之间参数的传递和同步问题 ，以及由大量数据跨云传输的时
间开销导致模型训练速度慢的问题 。为了提升训练速度 ，训
练任务被拆分到多个不同的算力集群上 。利用这些集群的算
力，可以实现对任务的并行处理 。根据不同的任务需求和场
景，跨云训练可以采用不同的并行策略 ，包括数据并行 、模
型并行和流水线并行等 。
数据并行是提升训练速度的一种并行策略 ，能够将训练
任务切分到多个算力集群上 。每个集群维护相同的模型参数
和计算任务 ，只是处理不同的批数据 。通过这种方式 ，全局
的数据被分配到不同的进程 ，从而减轻单个集群上的计算和
存储压力 。
模型并行主要用于模型太大 、无法在单个设备上加载的
场景 ，对计算图按层切分以减少单个存储的容量需求 ，每个
集群只保留模型的一部分 。因此 ，多个算力集群可以共同训
练一个更大的模型 。
当模型并行在某个集群进行计算时 ，其余集群都会处于
闲置状态 ，这样会极大地降低整体的使用效率 。于是 ，在模
型并行的基础上 ，如图 1所示 ，把原先的批数据再划分成若
干个微批次 ，按流水线方式送入各个算力集群进行训练 ，也
就是流水线并行[3]。
当在跨云场景下进行大规模语言模型训练时 ，由于巨大
的数据量和参数规模 ，不论是对训练数据还是模型张量进行
切分 ，在进行跨云同步传输时都会产生较大的耗时 ，会影响
整体的训练速度 。由此可见 ，数据并行和模型并行这两种方
式能够支持的模型参数规模有限 。而流水线并行训练则将模
型参数按照层次进行拆分 ，把不同层的模型参数放到不同集
群中进行训练 。训练过程中不需要同步全部模型参数 ，集群
之间只需要串行传递训练过程的中间计算变量 。该方法受模
型参数规模影响较小 ，更适合大规模语言模型的跨云训练 。
1.2 跨云流水线并行的主要挑战及关键技术
跨云流水线并行和普通流水线并行的最大区别在于处理
通信数据的方式 。目前 ，普通流水线并行策略通常仅在单个
计算资源中心内部使用 ，这意味着
计算设备之间存在专用的高带宽网
络连接 。此时 ，通信代价极低 ，通
常可以忽略不计 。然而 ，当普通流
水线并行策略应用于跨云场景时 ，
计算设备之间的连接带宽远低于上
述连接 ，通信代价将显著增加 ，这
将极大地影响训练效率 。图1的左
图和右图分别展示了普通流水线并行和跨云流水线并行的处理流程 。
普通流水线并行的效率评价指标为并行空泡占用率比例
（parallelism bubble ration ） ，该比例越小代表效率越高 。假设
并行的阶段 （stage）数为 p，微批次的数量 （micro-batch ）
为m，每个 micro-batch 的前向和后向执行时间为 tf和tb，则
空泡率为 ：
bubbleration=p-1
m+p-1 。 （1）
而在跨云流水线并行中 ，会出现因为通信而导致的额外
空泡 。假设通信时间为 tt，在不做任何处理的情况下 ，前向
和后向的通信时间相等 ，此时空泡率为 ：
bubbleration=() p-1(tf+tb+2mtt)
m() tf+tb+() p-1(tf+tb+2mtt) 。 （2）
因此 ，跨云流水线并行所面临的主要挑战是如何提高训
练效率 ，即如何降低并行空泡的占用率 。从上述公式 （2）
中可以看出 ，在跨云场景中 ，与普通流水线并行不同 ，增加
微批次的数量并不一定会提高效率 ，需要根据实际情况进行
分析 ，并计算出最优的微批次数量 。此外 ，公式 （2）还表
明，缩短通信时间 、减少阶段数量均有助于降低空泡率 。特
别是由于通信时间的存在 ，阶段数量对空泡率的影响更为显
著。因此 ，减少阶段数量可以带来更大的收益 。下面我们将
从这两个方面介绍相关的技术 。
缩短通信时间的核心在于减少通信的数据量 。为此 ，可
以采用稀疏化 、量化和低秩训练等技术 。另外 ，阶段数量主
要受到节点总内存的限制 。如果能够降低训练占用的内存 ，
就可以使每个节点容纳更多的参数 ，从而有可能降低阶段
数。需要注意的是 ，在此处 ，以增加通信量为代价来降低内
存的方案并不适用 。
稀疏化的主要思想是 ，神经网络层的输出中绝对值较大
的数值通常承载了更多的信息量 。因此 ，将中间层数据中的
大多数数值变为 0就不会损失主要信息 。对此可以利用稀疏
化数据的表示方式来压缩数据 ，从而减少通信量和存储空间
图1 普通流水线并行和跨云流水线并行并行
空泡设备 4
设备 3
设备 2
设备 1设备 4
设备 3
设备 2
设备 1并行
空泡
前向过程 反向过程 梯度更新 通信过程
50