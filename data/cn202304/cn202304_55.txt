大规模语言模型的跨云联合训练关键技术 潘囿丞  等 热点专题
中兴通讯技术
2023  年 8 月    第 29 卷第  4 期   Aug . 2023    Vol . 29  No. 4的占用 。
量化则是将传输的中间结果从原本 32位比特的浮点数
映射到 8位或者更少比特表示的整型数据上 。这种方式可以
有效压缩通信数据 ，但是会带来额外的误差 ，进而会影响到
训练的精度 。因此 ，需要根据实际的数据分布情况来设计量
化的位数和方式 。
大型模型通常存在 “过参数化 ”的问题 ，即虽然模型的
参数众多 ，但实际上模型主要依赖于低秩维度的内容 。为
此，可以采用一些基于低秩分解的训练方法 ，例如低秩适应
（LoRA ）[4]算法 。该方法新增了一个先降维再升维的旁路 。
这样的设计可以天然地降低中间数据的维度 。将降维矩阵的
输出位置作为切分点也可以达到减
少通信时间的目的 。
2 一种面向大规模语言模型的
跨云训练方法
大规模语言模型的训练任务包
括语言模型预训练和下游任务微调
两个阶段 。为了应对跨云模型训练
的挑战 ，本文中我们将介绍一种基
于跨云大模型训练框架 “星云 ”[5]
的预训练和微调方法 。如图2所示 ，
“星云 ”是一个专门面向云际环境的
深度学习模型统一训练框架 ，该框
架包含了任务层 、训练优化层 、并
行计算层 、通信优化层 、安全和隐
私层 、管理和调度层以及云硬件层
等7个功能层 ，支持在低带宽网络
环境下 ，利用不同算力集群的异构
算力进行大模型的跨云训练 ，在通
信优化方面采用了参数稀疏化 、量
化以及低秩分解等有效技术来确保
集群间信息传输的轻量化和最小化
模型精度损失 ，并主要采取流水线
并行的方式来实现在多个算力集群
间的并行计算 。
2.1 多语言大模型的跨云预训练
方法
针对多语言模型预训练任务 ，
我们基于 “星云 ”实现了一套支持
跨云多源数据训练的多语言模型预训练方案 ，如图 3所示。为了优化训练过程 ，该方案参考
ELECTRA[6]架构设计了一种适合跨云使用的模型架构 ，由生
成器 （Generator ）和判别器 （Discriminator ）两部分组成 。
其中 ，生成器根据输入内容生成对应的字符序列 ，判别器则
对生成的字符序列进行判断 ，以达到优化训练的目的 。
在模型训练过程中 ，生成器只需要将输出的字符序列单
向传递给判别器 。当进行跨云训练时 ，生成器和判别器会被
部署在不同的云集群上 ，此时生成器只需向判别器传输字符
串序列即可 。在这个过程中 ，所需的数据传输量较少 ，带宽
需求也较低 ，这有利于跨云大模型的训练 。此外 ，通过共享
生成器和判别器间的词表 、跨云只传输字符 ID序列的方式
CPU：中央处理器      GPU ：图形处理器      NPU ：神经网络处理器
图2 “星云 ”的框架结构示意图
图3 基于“星云 ”的跨云模型预训练框架任务层
训练优化层
并行计算层
通信优化层
安全和隐私层
管理和调度层
云硬件层星云优化器预训练 微调
深度学习模型训练优化策略
云际并行计算策略
参数稀疏化 量化 低秩分解 ……
集群内安全计算 隐私保护 集群间安全通信 ……
GPU 集群 NPU 集群 CPU 集群 ……智算网络统一资源调度与管理平台
被替换字符预测 新语言模型
判别器
（ERNIE-M ）
生成的输入字符序列
生成器 1
（ERNIE-M ）
语言集群 1生成器 2
（ERNIE-M ）
语言集群 2生成器 n
（ERNIE-M ）
语言集群 n
输入字符序列
多语言无监督数据语言
集群 1语言
集群 2语言集群
n-1语言
集群 n……云集群 A1 云集群 A2 云集群 Ai 云集群 An语言集群 n……
云集群 A云集群 B
高带宽环境
低带宽环境
高带宽环境
51