大规模语言模型的跨云联合训练关键技术 潘囿丞  等 热点专题
中兴通讯技术
2023  年 8 月    第 29 卷第  4 期   Aug . 2023    Vol . 29  No. 4语言生成微调训练和基于编码器架构的自然语言理解微调
训练 。
2.2.1 针对自然语言生成任务的微调
针对基于编码器 -解码器架构的
自然语言生成模型 ，本文以机器翻
译任务为例 ，参照 ABNet[13]模型架
构设计 ，实现基于 “星云 ”的跨云
机器翻译模型微调训练 。ABNet 是
一种用于微调训练的模型架构 ，在
编码器和解码器的各个子层之间插
入需要训练的适配器模块 。在训练
过程中 ，预训练模型的参数被冻结 。
该微调方法利用预训练语言模型的
知识 ，但不调整预训练模型的参数 。
如图6所示，针对源语言和目标语
言的预训练模型分别被部署在两个
云集群中 。
在模型训练时 ，每进行一步前
向计算和反向传播 ，编码端和解码
端都需要进行一次跨云中间数据传
输。数据传输量与数据批处理大小
（B） 、序列长度 （S） 、隐藏层维度
（H）等因素相关 。需要传递的数据
规模如公式 （3）所示 ：
Data size=B×S×H。 （3）
在微调训练过程中 ，数据传输占用了大量的网络带宽资源 。传输时间的长短对训练速度的
影响很大 。当网络带宽过低时 ，跨云训练就无法达到加速训
练的目的 。因此 ，为了提高模型的训练速度 ， “星云 ”框架
从云间通信和并行训练两个方面进行综合优化 。
为了解决在训练过程中数据传输量大 、传输时间长的问
题，针对需要跨云传输的中间数据 ，可以采用压缩通信的策
略进行优化 ，以减少单次传输的数据量 。可采用的压缩通信
方法主要包括量化 、稀疏化 、低秩分解等 。为了减小压缩通
信对模型精度的影响 ，可以组合使用不同的压缩策略 ，并在
训练的不同阶段采用不同的压缩传输策略 。
为了解决在模型训练过程中由串行计算导致的资源利用
率不高的问题 ， “星云 ”采用并行优化策略来优化训练过程 。
在云集群间采用流水线并行 ，云集群内采用数据并行的方
式，采用多微批次以流水线并行的方式在云集群间执行计算
和数据传输任务 ，可以减少同一时刻资源的停等 ，提高参与
训练各资源的利用率 。ABNet 架构在跨云环境的部署及并行
计算方式 如图 7所示。
为了进行跨云集群模型微调的实验 ，我们选择 IWSLT ’
图5 单云训练和跨云训练损失对比
图6 基于 ABNet 的跨云微调训练方法
图7 跨云微调训练算力互联及并行计算方式GPU：图形处理器0               20 000           40 000           60 000        80 000
训练步数训练损失8
7
6
5
4
3
2
1单云集群训练损失
跨云集群训练损失
前馈层 ×2适配器模块
预训练语言模型模块
通信优化
适配器子层
前馈层
自注意力层
源语言训练模型子层
源语言云集群前馈层适配器子层
交叉注意力层
前馈层
自注意力层
目标语言训练模型子层×N
目标语言云集群×N
云集群 S
编码器
数据并行GPU 0
GPU 7云集群 T
解码器
数据并行GPU 0
GPU 7
流水线并行前向计算通信
反向传播通信
53