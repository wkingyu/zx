大规模语言模型的跨云联合训练关键技术 潘囿丞  等 热点专题
中兴通讯技术
2023  年 8 月    第 29 卷第  4 期   Aug . 2023    Vol . 29  No. 4将模型拆分在第 11层，然后针对不同的 r值（分别为 8、16
和32）进行测试 ，结果 如表 3所示。特别地 ，在r等于 8的
情况下 ，传输数据量降为原有的 1/96，同时精度维持在原有
模型的相当水平 。
通过跨云场景的模型微调训练实验验证 ，我们证实了跨
云微调的可行性 。用户可以利用分布在不同云集群上的预训
练模型来微调目标任务模型 ，并通过复用已有模型的知识来提升模型性能 。这比仅使用自身数据训练模型更为优越 。由
于模型被拆分成多个部分 ，用户可以将模型的底层部分置于
可信集群上 ，从而确保其他集群无法获得标注数据 ，保障用
户标注数据的安全性 。
3 跨云训练算力互联及未来场景
生成算法 、预训练模型 、多模态等技术的融合催生了以
ChatGPT 为代表的人工智能生成内容 （AIGC ）的爆发 ，进而
带来了高算力需求 。以ChatGPT 为例 ，它使用了 10 000块
A100 GPU 进行训练 。此外 ，它的部署成本也很高 ，根据国
盛证券报告估算 ，它的每日咨询量对应的算力需求达到了上
万块 A100。所以 ，利用跨云训练可以将广泛分布的算力结
合起来 ，这是应对大模型对算力高需求的一种解决方案 ，从
而有效应对算力对大模型训练的制约 。同时 ，跨云训练可以
利用闲散算力 ，有效解决碎片化问题 ，提高云集群资源的利
用率 。
除了算力限制 ，与个人信息强相关的应用 ，例如语音助
手、心理咨询等 ，也关注隐私保护问题 。跨云训练机制具备
较好的隐私保护能力 。用户可以通过构建本地设备与云的协
同训练来实现个人信息在本地处理 、云端提供算力的方式 ，
从而保证个人信息不被泄露 。
4 结束语
本文的研究表明 ，在跨云环境下进行大规模语言模型训
练是可行的 ，是一种提高算力利用率的方案 。通过采用模型
分割 、拆分学习 、跨云协同 、压缩通信和模型复用等关键技
术，该方案能够有效解决跨云训练过程中可能出现的算力和
数据不足的问题 ，并提高训练速度和效率 。这些技术在自然
语言处理领域的应用将有望带来更为精准和高效的文本处理
和语义分析结果 ，并具备较好的隐私保护能力 ，为智能化应
用和人机交互等领域的发展提供有力的支持 。
致谢
感谢百度飞桨团队吴志华和巩伟宝 ，以及哈尔滨工业大
▼表3 11层拆分微调结果 （k表示 1 000）
基线模型
r=8
r=16
r=32SST- 2
（67k）
92.54
92.43
92.31
92.77QNLI
（105k）
91.24
90.98
91.22
91.04MNLI
（364k）
84.56
83.98
84.33
84.27QQP
（91.2k）
90.73
90.93
90.75
90.99CoLA
（8.5k）
55.3
57.13
57.35
57.87RTE
（2.5k）
66.06
64.25
62.09
62.81STS-B
（7k）
88.38
86.46
86.78
87.46MRPC
（3.7k）
85.33
84.81
83.47
84.23SQuAD
（88k）
88.25
88.33
88.75
88.56
（a）SST- 2
图9 基于低秩分解的跨云微调（b）QNLI层索引1   2    3   4    5   6    7   8   9  10 11 12基线
低秩分解的微调训练准确率 /%92
90
88
86
84
82准确率 /%90
80
70
60
50
层索引1   2    3   4    5   6    7   8   9  10 11 12基线
低秩序分解的微调训练
注：QNLI 、SST- 2是两种数据集
55