ZTE COMMUNICATIONS
September  2023  Vol.21 No.3JI Yuhe , HAN Jing , ZHAO Yongxin , ZHANG Shenglin , GONG Zican Research Papers   Log Anomaly Detection Through GPT -2 for Large Scale Systems
machine ID , etc.) with markers and extract the template of the 
log to distinguish between logs . In this paper , we adopt 
Drain[11], a heuristic log parsing algorithm , to parse logs . Drain 
parses the template of logs by maintaining a tree , which keeps 
the leaves as log groups with different heuristic rules in inter ‐
nal nodes . New logs would be distributed into log groups in 
leaves according to the corresponding tree path .
2) GPT -2. The GPT algorithm works based on the decoder 
of the transformer , and it pre -trains unsupervisedly on massive 
corpus data and then finetunes the model according to the spe ‐
cific tasks . GPT -2 removes the finetune step and increases the 
size of the training dataset and model , which makes the model 
perform well in multi -tasking .
3 Approach
In this section , we will introduce our log anomaly detection 
framework . Inspired by GPT -2, we employ transformer decod ‐
ers to encode the normal pattern of system logs . Our framework 
is divided into four main parts : log parsing , sentence vector gen ‐
eration , model training and detection , and the alerting strategy 
layer . For Challenge 1, we embed each log template into a vec ‐
tor representing the semantic information . We use vectors in ‐
stead of tokens as input to GPT -2, so the model can capture 
both semantic and sequence information to encode normal pat ‐
terns . For Challenge 2, we design an alarm strategy layer for the 
framework , which can filter out false positives by the statistical 
characteristics of log data . The structure of the framework of our 
work is shown in Fig . 2. The raw logs collected are first trans ‐
formed into structured logs by the log parser . Then a sentence 
vector generation model will be used to generate sentence vec ‐
tors for each extracted log template .
3.1 Log Parser
System logs obtained from the database are semi -structured 
text, which is difficult to be used for model training . Therefore , 
before processing logs , we need to use a parser to convert logs 
into structured text . In this paper , we use Drain as our log 
parser , which can divide logs into templates and dynamic vari ‐
ables . Drain is an online log template miner , employing a parse 
tree with a fixed depth , and it can extract templates and vari ‐
ables from a stream of log messages . Drain first sorts logs into 
different buckets by length and then matches similar portions of 
the log from front to back in each bucket . Eventually , logs be ‐longing to the same template will end up on the same leaf node . 
The structure of the depth -fixed tree is shown in Fig . 3. In the 
figure , sys is an abbreviation for system , and HEX and NUM 
are variables matched during the parsing process , representing 
hexadecimal and decimal numbers , respectively .
3.2 Sentence Vector Generation
To make GPT -2 better at encoding normal patterns , we gen ‐
erate semantically relevant sentence vectors for each log tem ‐
plate . We choose SBERT[12] as our embedding model , which 
has been widely used in text similarity calculation and sen ‐
tence classification problems and has achieved excellent re ‐
sults. SBERT modifies the pre -trained BERT model , and it 
implements Siamese or triplet net frameworks to generate se ‐
mantically meaningful sentence embedding . Sentence vectors 
generated by templates with similar meanings have smaller co ‐
sine distances or Euclidean distances . Table 1 shows the Eu ‐
clidean distance of sentence vectors in five log templates .
3.3 Detection Model
1) Model framework . We choose GPT -2 as our log anomaly 
detection model in this work . GPT -2 is an unsupervised Natu ‐
ral Language Processing (NLP) model stacked from the trans ‐
former’s decoders . The structure of GPT -2 is shown in Fig . 4. 
Each decoder has a masked self -attention layer , a feed -
forward neural network , and two normal layers . The structure 
of each decoder is the same , but each module maintains sepa ‐
rate parameters . Different from the ordinary self -attention 
layer , the masked self -attention layer does not allow a node to 
▲Figure 2. Approach overview EoF: end -of-file▲Figure 3. Structure of the depth -fixed tree
Templates
httprequest except <*> permission denied
httprequest except <*> <*> permission denied
httprequest except <*> no such file or directory
httprequest except <*>
httprequest except EoF occurred in violation of protocol
httprequest except <*> connection reset by peerEuclidean Distance
-
0.147 629 340 284 133 4
0.595 852 332 701 891 4
0.621 201 472 867 456 3
0.838 852 193 154 771 3
0.880 359 580 380 884 6▼Table 1. Euclidean distance of sentence vectors of similar semantic 
templates
Log data Structured log
Error logsLog parser
Alarm
strategy layerEmbedding 
model
ModelRoot
Length : 4 Length : 5 Length : 6
sys mmcs
<NUM> …
Log groups
Log 
groupLog 
group…
Log group
template : sys <NUM> at <HEX> mask
<HEX>
ID: [1,3,34,56,57,59,…]
Sentence
vectors
72