ZTE COMMUNICATIONS
September  2023  Vol.21 No.3JI Yuhe , HAN Jing , ZHAO Yongxin , ZHANG Shenglin , GONG Zican Log Anomaly Detection Through GPT -2 for Large Scale Systems    Research Papers
get information from subsequent nodes . This feature makes 
the model perform well in sequence prediction tasks .
2) Input representation . Like most NLP models , GPT -2 
looks up the embedding vector corresponding to the word from 
the embedding matrices , and the embedding matrices are also 
part of the model training results . GPT -2 maintains two em ‐
bedding matrices : a token embedding matrix and a position 
embedding matrix . Each row of the token embedding matrix is 
a vector that represents a token , and these vectors are ran ‐
domly initialized and continuously adjusted during training . 
Each row of the position embedding matrix represents the posi ‐
tion information of the token . Tokens in the same position in 
different sentences have the same position embedding vector . 
Each template embedding inputted into GPT -2 is the sum of 
position embedding and token embedding . In our framework , 
we initialize the token embedding matrix with sentence vec ‐
tors generated by SBERT , which unlike the original random 
initialization , can make the model catch the initial semantic 
information of log templates more accurately . Besides , this ap ‐
proach gives operations more control over the model . We can 
adjust the dimension or generation method of sentence vectors 
according to the characteristics of logs and actual business re ‐
quirements .
3) Model training . The core concept of GPT -2 is language 
modeling . Language modeling refers to distribution estimation 
from a group of unsupervised samples (x_1, x_2, x_3, …). 
Each sample consists of symbol sequences of variable length 
(s_1, s_2, s_3, …). Because there are explicit sequential rela ‐
tionships between phrases in natural languages , language mod ‐
eling typically decomposes the joint probability of symbols as 
a product of conditional probabilities .
p()x= ∏
i=1n
p( ) si|s1, s2,…,sn-1. (1)
Transforming the above equation into a logarithmic form , the goal of the language model is to maximize the probability 
of the following equation .
p(x)= ∑
i=1n
log p( ) si|s1, s2,…,sn-1; θ , (2)
where n is the length of the language sequence , and the condi ‐
tional probability p is modeled by the neural network with pa ‐
rameter θ. These parameters are trained by the stochastic gra ‐
dient descent .
The input of the decoder at the first layer consists of the to ‐
ken embedding vector and position embedding vector of log 
templates . The output of each decoder is processed from the 
previous layer ’s output . The output probability obtained by 
the model can be expressed as follows :
h0=XWe+ Wp,
hl= Decoder()hl-1  ∀l∈[]1,n ,
p()x=softmax(hnWT
e), (3)
where hl represents the output of the l-th layer decoder , X is 
the matrix composed of the unique thermal encoding of the in ‐
put log sequence , We is the token embedding matrix , and  WP 
is the position embedding matrix . To maximize the prediction 
probability , the model adjusts the decoder parameters of each 
layer during the learning process .
3.4 Alarm Strategy
Analyzing the actual data , we find that in the production en ‐
vironment , the logs printed by the machine are not always se ‐
quential . The main differences between these noises and se ‐
vere systems are as follows : logs corresponding to noise ap ‐
pear frequently and usually have a certain seasonality , while 
severe anomalies occur infrequently and are difficult to pre ‐
dict. Based on the above observation , we use frequency and 
periodicity as criteria to determine whether model error report ‐
ing is noisy or a true anomaly .
For the abnormal log templates detected by the model , we 
first calculate the time interval of their occurrence in the train ‐
ing data and then use the auto -correlation coefficient to ana ‐
lyze whether the time interval of template occurrence has a 
specific pattern . The auto -correlation coefficient is a common 
parameter for finding repetitive patterns (e.g., periodic signals 
masked by noise ) and is often used in signal processing prob ‐
lems. The sequence consisting of the auto -correlation coeffi ‐
cients is known as the auto -correlation function . For the ob ‐
tained template interval sequence , its autocorrelation function 
is calculated , and the peak of the function is the possible pe ‐
riod of the corresponding template . If the value of the function 
at a certain time is higher than a threshold value set based on 
expert experience , we consider the template to be periodic 
and its basic impossibility to be an error log template .
Then , for non -periodic templates , we go through the statis ‐▲Figure 4. Structure of generative pre -training -2 (GPT -2)Decoder
Decoder 3
Decoder 2
Decoder 1Normal layer Decoder 4
Feed forward neural network
Normal layer
Masked self -attention
InputOutput
73