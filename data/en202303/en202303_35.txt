ZTE COMMUNICATIONS
September  2023  Vol.21 No.3SHEN Jiahao , JIANG Ke , TAN Xiaoyang Boundary Data Augmentation for Offline Reinforcement Learning    Special Topic
method . To be specific , we first train the generator GB via Eq . (4), 
and then we enter the offline RL loop , where we update the 
policy network π via Eq . (6) and the Q networks Q via Eq . (7) 
alternately . Finally , we output the policy network π for the 
testing stage .
5 Experiments
This section starts with an introduction to the datasets used 
in our research . Subsequently , the efficacy of our proposed 
framework is demonstrated through its assessment on Datasets 
for Deep Data -Driven Reinforcement Learning (D4RL) bench ‐
marks . Further , an examination of the behavior of our genera ‐
tor and its impact on the new policy is conducted . Finally , a 
sensitive analysis is performed to elucidate the contribution of 
each parameter .
5.1 Datasets
The experiments presented in this study are carried out 
on the OpenAIGYM subset of the D 4RL[34] tasks . For perfor ‐
mance evaluation , we utilize datasets that are a combination 
of multiple policies , namely medium , medium -replay , 
medium -expert , and expert . To ensure the robustness of our 
findings , we conduct all experiments at four distinct random 
seeds .
5.2 Comparative Study
To demonstrate the superiority of our proposed framework , 
we conduct a comparative analysis with several state -of-the-
art algorithms , including behavior cloning (BC), BEAR[17], Soft 
Actor -Critic (SAC)[35], twin -delayed deep deterministic policy 
gradient (TD3)+BC[36], and CQL[8]. In particular , we obtain the 
results for CQL and BEAR using our implementation , while 
the effects of BC and SAC are taken from Clean Offline Rein ‐forcement Learning (CORL )37] and MCQ[20], respectively . Addi ‐
tionally , we obtain the results for TD 3+BC from its original 
publication . Table 1 presents the results , with the highest 
mean value being denoted in bold .
As is evident from the results presented in Table 1, our pro ‐
posed algorithm outperforms the other state -of-the-art ap ‐
proaches in the medium , medium -expert , and medium -replay 
datasets , which exhibit diverse characteristics . In comparison 
with the basic version of CQL , our approach demonstrates su ‐
perior performance in estimating OOD data . However , in an 
expert setting , although we employ a generative network to 
simulate OOD data , with the constraint on the OOD data , our 
approach performs better than the original CQL but worse 
than TD 3+BC when employing behavior cloning in the half 
cheetah task . Overall , these findings highlight the excellent ef ‐
ficiency of our proposed framework in both complex tasks and 
expert environments .
Fig. 2 displays the performance of BCQL and CQL during 
the training process . As illustrated by the curves , our pro ‐
posed algorithm outperforms both BC and basic CQL in the 
medium , medium -replay , and medium -expert environments , 
owing to the utilization of augmented data . In contrast , BC em ‐
ploys data obtained through behavior cloning . In the expert en ‐
vironment , the regularization imposed on the generated low -
confidence data leads to a minimal impact on the performance 
of high -confidence data .
5.3 Behaviors of Generator
The present study employs a GAN -based generator to gen ‐
erate data in various environments , and the real and gener ‐
ated states are visualized , as shown in Fig . 3. The generated 
data is observed to be irregular yet maintained its validity in 
comparison to the original data . To assess the effectiveness 
▼Table 1. Performance of BCQL and prior methods on MuJoCo tasks from D 4RL, on the normalized return metric (the highest means are bolded )
Task Name
Halfcheetah -medium -v2
Hopper -medium -v2
Walker 2d-medium -v2
Halfcheetah -medium -replay -v2
Hopper -medium -replay -v2
Walker 2d-medium -replay -v2
Halfcheetah -medium -expert -v2
Hopper -medium -expert -v2
Walker 2d-medium -expert -v2
Halfcheetah -expert -v2
Hopper -expert -v2
Walker 2d-expert -v2
Total averageBC
42.4±0.2
53.5±2.0
63.2±18.8
35.7±2.7
29.8±2.4
21.8±11.7
56.0±8.5
52.3±4.6
99.0±18.5
91.8±1.5
107.7±0.7
106.7±0.2
63.3BEAR
37.1±2.3
30.8±0.9
56±8.5
36.2±5.6
31.1±7.2
13.6±2.1
44.2±13.8
67.3±32.5
43.8±6.0
100.2±1.8
108.3±3.5
106.1±6.0
56.2SAC
55.2±27.8
0.8±0.0
-0.3±0.2
0.8±1.0
7.4±0.5
-0.4±0.3
28.4±19.4
0.7±0.0
1.9±3.9
-0.8±1.8
0.7±0.0
0.7±0.3
7.9TD3+BC
48.3±0.3
59.3±4.2
83.7±2.1
44.6±0.5
60.9±18.8
81.8±5.5
90.7±4.3
98.0±9.4
110.1±0.5
96.7±1.1
107.8±7
110.2±0.3
82.7CQL
47.1±0.2
64.9±4.1
80.4±3.5
45.2±0.6
87.7±14.4
79.3±4.9
96±0.8
93.9±14.3
109.7±0.5
96.3±1.3
109.5±14.3
108.5±0.5
83.8BCQL
47.1±0.7
66.1±5.2
84.6±2.5
46.1±1.5
93.9±11.8
82.5±7.3
97.5±3.2
95.9±13.3
110.2±1.0
98.4±3.2
111.7±8.3
109.7±1.0
87.0
BC: behavior cloning 
BCQL : Boundary Conservative Q Learning 
BEAR : bootstrapping error accumulation reduction 
CQL: Conservative Q -Learning D4RL: Datasets for Deep Data -Driven Reinforcement Learning 
SAC: Soft Actor -Critic 
TD3: twin -delayed deep deterministic policy gradient
33