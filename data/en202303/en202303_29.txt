ZTE COMMUNICATIONS
September  2023  Vol.21 No.3YU Junpeng , CHEN Yiyu A Practical Reinforcement Learning Framework for Automatic Radar Detection    Special Topic
1 s or 10 s), we assume that the environment will remain 
largely unchanged even if the radar scans every alternate turn . 
As for the reward setting issue , we alternate between a circle 
set as a frequency point generated by the algorithm and the 
next circle as a fixed frequency point . This approach provides 
a relatively standardized reward for the reinforcement learning 
algorithm , rt(a)=SNR(st,a)-SNR(st,afix).
Evaluation setting : For offline data , actions are expert strat ‐
egies , and we compare the overlap between algorithm output 
actions and offline data actions . For online learning , consider ‐
ing the practical application , we involve relevant radar opera ‐
tion experts to compare the algorithm ’s parameter control re ‐
wards with the expert ’s parameter control rewards . We ask 
both the algorithm and experts to test each other on the same 
task, compare the cumulative rewards of the algorithm with 
the cumulative rewards of the experts , and take the average of 
three experiments . To benchmark real -world sample -limited 
applications , in the online training stage , the sample number 
is limited to 10 rounds , i.e. 10 000 steps .
Implementation : We employ proximal policy optimization 
(PPO) as the reinforcement learning algorithm during the on ‐
line training phase . We use MATLAB as it supports the Radar 
Toolbox to construct a simulation environment . We achieve 
code communication between Python and MATLAB via the 
user datagram protocol (UDP ). The environment and algo ‐
rithms ran on a 2.5 GHz CPU and a single NVIDIA GeForce 
RTX 3080  graphics card . For offline training data , we have ex ‐
perts control the selection of radar parameters in the task , but 
we also aim to cover as many action intervals as possible , 
thereby obtaining data with a total of 100 000 steps with a 
wide distribution of action .
Experimental results : After achieving convergence in the of ‐
fline training phase , the action similarity between the decision 
model and offline data is 99%. In online tests , the average cu ‐
mulative reward of the proposed method reaches 91% of the 
experts’ method , and the performance of a random policy is 
unstable and obviously weaker . Detailed online testing results 
with average cumulative reward are shown in Table 1. Addi ‐
tionally , we observe that the decision model can control differ ‐
ent parameters for different targets , and it tends to favor some 
commonly used radar parameters . The experimental results in ‐
dicate that the decision model has learned the preliminary ra ‐
dar control policy , but the potential of deep learning may not 
be fully exploited due to the limitation of the training sample 
size. According to our experience and expert judgment , our 
method can act as humans in basic radar automatic detection , and therefore has the potential to be applied in practical radar 
operation tasks . In future research , we will focus on further en ‐
hancing the effectiveness of reinforcement learning and aim to 
apply it to actual radar .
5 Conclusions
In this paper , we have presented a novel practical approach to 
radar operation that leverages the power of reinforcement learn ‐
ing. By integrating offline reinforcement learning and meta -
reinforcement learning methods , we have developed a practical 
radar operation reinforcement learning framework that can 
quickly adapt to unseen real -world tasks . Our experimental re ‐
sults have demonstrated the ability to act as humans in basic ra ‐
dar automatic detection with real -world settings , thereby validat ‐
ing our approach . Our work not only addresses the current chal ‐
lenges in radar operation but also paves the way for the practical 
application of reinforcement learning in radar operation . The 
proposed method has the potential to revolutionize radar detec ‐
tion by enhancing its efficiency , precision , and automation . Fu‐
ture work will focus on further refining our framework and ex ‐
ploring its application in real -world radar systems .
References
[1] GENG Z , YAN H , ZHANG J , et al . Deep -learning for radar : a survey [J]. IEEE ac ‐
cess, 2021 , 9: 141800 -141818 . DOI :10.1109 /ACCESS .2021 .3119561
[2] AZIZ M M , MAUD A R M , HABIB A . Reinforcement learning based techniques 
for radar anti -jamming [C]//International Bhurban Conference on Applied Sciences 
and Technologies (IBCAST ). IEEE , 2021 : 1021–1025 . DOI : 10.1109 /
IBCAST 51254 .2021 .9393209
[3] SUTTON R S , BARTO A G . Reinforcement learning : an introduction (2nd ed ) [M]. 
Cambridge , USA : MIT press , 2018
[4] SILVER D , HUANG A , MADDISON C J , et al . Mastering the game of Go with 
deep neural networks and tree search [J]. Nature , 2016 , 529(7587 ): 484–489. 
DOI: 10.1038 /nature 16961
[5] VINYALS O , BABUSCHKIN I , CZARNECKI W M , et al . Grandmaster level in 
StarCraft II using multi -agent reinforcement learning [J]. Nature , 2019 , 575(7782 ): 
350–354. DOI : 10.1038 /s41586 -019 -1724 -z
[6] LI J J , KOYAMADA S , YE Q W , et al . Suphx : mastering mahjong with deep rein ‐
forcement learning [EB/OL ]. [2023 -04-16]. https ://arxiv .org/abs/ 2003 .13590
[7] DEGRAVE J , FELICI F , BUCHLI J , et al . Magnetic control of tokamak plasmas 
through deep reinforcement learning [J]. Nature , 2022 , 602(7897 ): 414–419. DOI : 
10.1038 /s41586 -021 -04301 -9
[8] SCHRITTWIESER J , ANTONOGLOU I , HUBERT T , et al . Mastering Atari , 
Go, chess and shogi by planning with a learned model [J]. Nature , 2020 , 588
(7839 ): 604–609. DOI : 10.1038 /s41586 -020 -03051 -4
[9] MNIH V , KAVUKCUOGLU K , SILVER D , et al . Human -level control through 
deep reinforcement learning [J]. Nature , 2015 , 518(7540 ): 529–533. DOI : 
10.1038 /nature 14236
[10] WANG Z Y , SCHAUL T , HESSEL M , et al . Dueling network architectures for 
deep reinforcement learning [C]//The 33rd International Conference on Interna ‐
tional Conference on Machine Learning . ACM , 2016 : 1995–2003 . DOI : 10.5555 /
3045390 .3045601
[11] HAUSKNECHT M , STONE P . Deep recurrent Q -learning for partially observable 
MDPs [J]. AAAI fall symposium , 2015 : 29–37
[12] LILLICRAP T P , HUNT J J , PRITZEL A , et al . Continuous control with deep rein ‐
forcement learning [EB/OL ]. [2023 -04-16]. https ://arxiv .org/pdf/ 1509 .02971 .pdf. 
DOI: 10.1016 /S1098 -3015 (10)67722 -4
[13] SCHULMAN J , WOLSKI F , DHARIWAL P , et al . Proximal policy optimization al ‐
gorithms [EB/OL ]. (2017 -08-28) [2023 -04-16]. https ://arxiv .org/abs/ 1707 .06347▼Table 1. Online testing results
Test
Trial 1
Trial 2
Trial 3
AverageRandom Policy
−9.24
12.78
6.34
3.29Proposed
24.32
28.77
25.38
26.16Experts
26.14
29.33
30.85
28.77
27