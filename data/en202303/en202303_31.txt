ZTE COMMUNICATIONS
September  2023  Vol.21 No.3SHEN Jiahao , JIANG Ke , TAN Xiaoyang Boundary Data Augmentation for Offline Reinforcement Learning    Special Topic
Boundary Data Augmentation for Boundary Data Augmentation for 
Offline Reinforcement LearningOffline Reinforcement Learning
SHEN Jiahao1,2, JIANG Ke1,2, TAN Xiaoyang1,2
(1. College of Computer Science and Technology , Nanjing University of 
Aeronautics and Astronautics , Nanjing 211106 , China；
 2. MIIT Key Laboratory of Pattern Analysis and Machine Intelligence , 
Nanjing 211106 , China )DOI: 10.12142 /ZTECOM .202303005
https ://link .cnki.net/urlid/ 34.1294 .TN.20230814 .1519 .002.html, published 
online August 14, 2023
Manuscript received : 2023 -07-05
Abstract : Offline reinforcement learning (ORL ) aims to learn a rational agent purely from behavior data without any online interaction . One 
of the major challenges encountered in ORL is the problem of distribution shift , i.e., the mismatch between the knowledge of the learned 
policy and the reality of the underlying environment . Recent works usually handle this in a too pessimistic manner to avoid out -of-distribution 
(OOD ) queries as much as possible , but this can influence the robustness of the agents at unseen states . In this paper , we propose a simple but 
effective method to address this issue . The key idea of our method is to enhance the robustness of the new policy learned offline by weakening 
its confidence in highly uncertain regions , and we propose to find those regions by simulating them with modified Generative Adversarial Nets 
(GAN ) such that the generated data not only follow the same distribution with the old experience but are very difficult to deal with by them ‐
selves , with regard to the behavior policy or some other reference policy . We then use this information to regularize the ORL algorithm to pe ‐
nalize the overconfidence behavior in these regions . Extensive experiments on several publicly available offline RL benchmarks demonstrate 
the feasibility and effectiveness of the proposed method .
Keywords : offline reinforcement learning ; out‐of‐distribution state ; robustness ; uncertainty
Citation  (Format 1): SHEN J H , JIANG K , TAN X Y . Boundary data augmentation for offline reinforcement learning [J]. ZTE Communications , 
2023 , 21(3): 29–36. DOI : 10.12142 /ZTECOM .202303005
Citation  (Format 2): J. H. Shen , K. Jiang , and X . Y. Tan, “Boundary data augmentation for offline reinforcement learning ,” ZTE Communica ⁃
tions, vol. 21, no. 3, pp. 29–36, Sept . 2023 . doi: 10.12142 /ZTECOM .202303005 .
1 Introduction
Reinforcement learning (RL) is one of the major 
branches of machine learning that has been success ‐
fully applied in various fields in recent years , such as 
power grid control[1], recommendation systems[2], and 
robotics[3]. RL training usually involves a large number of try -
and -error interactions with underlying systems . However , such 
“interaction hungry ” behavior could have a serious negative 
impact on many real -world applications , especially when the 
online data are either costly or dangerous to collect , e.g., in 
healthcare[4], autonomous driving[5], and so on . To address this 
problem , the idea of offline RL (ORL ) is to learn a new policy 
only from data based on offline dataset without online interac ‐
tion. Unfortunately , the direct employment of the common off -
policy strategy often fails to achieve the same level of perfor ‐
mance as in the online setting[6–7].
The extrapolation error from out -of-distribution (OOD ) ac‐tions is generally thought of as the main reason responsible for 
the aforementioned performance degradation[6]. The OOD ac ‐
tions here mean the actions taken by a model or system that is 
outside the range of the examples it was trained on . Since the 
dataset used for ORL training is generated by a behavior 
policy different from the new policy , possibly working in a dif ‐
ferent environment as well , it is not always possible for the 
agent to generalize the knowledge learned from the data to un ‐
seen real online situations . This is not uncommon in practice 
and usually manifests as the overestimated Q -value of OOD 
actions and such error compounds , leading to potentially dan ‐
gerous consequences . The problem is usually referred to as 
distribution shift . To address this , many recent works , such as 
Conservative Q -Learning (CQL)[8] and Implicit Q -Learning 
(IQL)[9], take a conservative strategy by trying to prevent the 
agent from taking overestimated OOD actions , with the idea 
that if most of the states visited are familiar to us , the chance 
of error and the subsequent error compounding phenomenon 
could be greatly reduced . However , taking OOD actions on ‐
line is almost inevitable , and avoiding the queries on them 
may significantly reduce the robustness of the agent . Actually , 
not all OOD data is non -generalizable[10].
This work is partially supported by the National Key R&D program of Chi ⁃
na under Grant No . 2021 ZD0113203  and National Science Foundation of 
China under Grant No . 61976115 .
29