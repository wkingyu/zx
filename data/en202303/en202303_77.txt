ZTE COMMUNICATIONS
September  2023  Vol.21 No.3JI Yuhe , HAN Jing , ZHAO Yongxin , ZHANG Shenglin , GONG Zican Log Anomaly Detection Through GPT -2 for Large Scale Systems    Research Papers
tion environment .
The complexity of Bob is much less than that of Ada , 
mainly because of less templates and a relatively fixed log se ‐
quence . However , due to problems such as network latency 
and data washout , the logs generated by the switch have a 
large number of errors and retry messages . In most cases , 
these error messages are not of concern because the program 
can be restored to normal after several retries and will not 
have a significant impact on the execution of the business . 
Also, these alarms of variable duration can disrupt the log se ‐
quence , making it more difficult to learn the normal pattern of 
the logs purely from frequency or sequence . The sequence 
confusion greatly affects the learning ability of LogAnomaly 
on this dataset , and NeuralLog has a lower Precision due to 
the report of the unimportant exceptions . Our framework cap ‐
tures the normal characteristics of logs from multiple perspec ‐
tives and designs alerting policies based on the frequency and 
periodicity of logs , thus achieving good performance on the 
Bob dataset .
The experimental results show that our framework has good 
performance for complex log datasets in practice . The analysis of 
log semantics makes the model show good robustness on poor 
stability sequences , and the addition of the alert policy layer re ‐
duces the model ’s disturbance to engineers and screens out 
some of the exceptions that can be fixed automatically .
2) RQ2: How effective is the sentence vector generation and 
alerting strategy layer in improving the effectiveness of the 
model ?
As mentioned in the previous sections , we add the sen ‐
tence vector matrix as well as the alert alarm strategy layer to 
GPT -2. In this RQ , we will verify the effect of the main parts 
of the framework on its effectiveness , by removing one or two 
components , namely the sentence vector (SV) and the alarm 
strategy (AS).
OM w/o SV & AS : We remove the sentence vector genera ‐
tion and alert policy layers from our framework . That is , we 
use only the GPT -2 model for sequence prediction to diagnose 
anomaly logs .
OM w/o AS : We remove the alarm strategy generation from 
our framework .
OM w/o SV : We remove the sentence vector layer from our 
framework .
OM: Anomaly detection work on logs using the completed 
framework proposed in this paper .
The experimental results in Table 4 show that the sentence 
vector generation part of the framework can improve the accu ‐
racy of the model to some extent and greatly enhance Recall . 
Because GPT -2 achieves better results in capturing the seman ‐
tic information of normal and abnormal templates after receiv ‐
ing the sentence vectors of the templates as prior knowledge . 
This is demonstrated by the fact that log templates containing 
the same abnormal keywords , the vector representations of 
which have a closer distance , are easily detected together in the anomaly detection stage . At the same time , log templates 
that symbolize normal patterns are more difficult for the model 
to detect as false positives because they often contain positive -
meaning words . Since the alarm strategy layer is built based 
on expert experience and has accurate filtering rules , it can fil ‐
ter out a large number of false abnormal logs and effectively 
improve the Recall of the model .
With the inclusion of both components , the effectiveness of 
our framework has been significantly improved . For complex 
logging environments , more accurate exception identification , 
very low FPs and a fairly high Recall can be achieved .
5 Related Work
As a kind of operational data , logs are widely used for sys ‐
tem anomaly detection in practice . To take advantage of the 
logs, previous work mainly focuses on detecting abnormal logs 
with artificial rules , which is not appropriate in scenarios with 
a large number of logs[5]. And deep learning is widely used in 
automatic anomaly detection in logs . DeepLog[3] uses LSTM to 
learn the normal pattern of the system and predict the next log 
template by log sequence . LogAnomaly[5] uses a word embed ‐
ding model to mine semantic information of log templates and 
learn the sequential patterns and quantitative relationships for 
logs with LSTM . LogRobust[13] represents the semantic infor ‐
mation of the log by word vector and takes advantage of the bi ‐
directional LSTM to learn the normal pattern of the log . 
OneLog[14] merges components (such as parsers and classifi ‐
ers) into a deep neural network to detect log anomalies . Log‐
Merge[15] learns the semantic similarity of multi -syntax logs to 
realize the transfer of log exception patterns across log types , 
which greatly reduces the overhead of exception annotation . 
Transformers could also be used to represent the semantics of 
the log and model the log sequence , and anomalies would be 
detected with learned information[10, 15–17]. GPT -2[10] is pro ‐
posed for unsupervised learning of text information based on 
transformers and performs well on text generation , text classifi ‐
cation , semantic judgment , etc.
6 Conclusions and Future Work
In practical scenarios , large -scale systems produce logs that 
are different from the vast majority of laboratory open -source 
datasets . The log sequence is more complex , and normal pat ‐
terns are harder to capture . In this work , we introduced a way 
▼Table 4. Experimental results
Approach
OM w/o SV & AS
OM w/o AS
OM w/o SV
OMAda
Precision
0.128
0.427
0.627
0.738Recall
0.835
1.00
0.807
1.00F1S
0.222
0.598
0.705
0.850Bob
Precision
0.510
0.718
0.833
0.857Recall
0.940
1
0.940
1.00F1S
0.661
0.836
0.883
0.923
AS: alarm strategy
OM: our methodSV: sentence vector
75