ZTE COMMUNICATIONS
September  2023  Vol.21 No.3JI Yuhe , HAN Jing , ZHAO Yongxin , ZHANG Shenglin , GONG Zican Log Anomaly Detection Through GPT -2 for Large Scale Systems    Research Papers
put the vectors into GPT -2 as the representation of templates . 
In this way , the models can study both sequential and seman ‐
tic features of input logs .
2) To address the second challenge , we design an alarm 
strategy layer for this framework . This step will analyze the sta ‐
tistical characteristics through the existing log data to effec ‐
tively reduce the impact of noise on model judgment and re ‐
duce the false positive rate .
The remainder of this paper is organized as follows . Section 
2 introduces the background of log anomaly detection . Section 
3 describes our approach . In Section 4, we present our experi ‐
mental design and results . Section 5 surveys related works 
and Section 6 concludes this paper .
2 Background
2.1 Logs Description
Logs, which are produced by the running program and con ‐
tain information generated from the logging module , are a type 
of semi -structured text . They reflect the running flow and real -
time status of the program , and can be used to detect and lo ‐
calize anomalies by operators . Logs consist of a structured 
part (constant ) and an unstructured part (variable ), as shown 
in Fig . 1. A structured part is fixed by the designer at the be ‐
ginning according to certain designed rules , and can reflect 
the event of the log , such as the level of logs (e.g., warning and 
error ), the logger name (e.g., root ) and so on . This part would 
not change into the same module . The unstructured part con ‐
tains specific information on logs and varies according to the 
input of the program and running status . An unstructured part 
would reflect the real -time status of the system , and it is essen ‐
tial to use this part to analyze the system and detect anomalies 
in the system . To make full use of the important features in 
logs, the semi -structured texts need to be parsed into the struc ‐
tured text with a parsing algorithm . The useless messages in 
the raw log would be filtered out , and the valuable information 
would be extracted out from the left information to train the 
model and detect the anomaly .
Generally speaking , the logs generated by normally operat ‐
ing hardware and software systems have a good regularity . 
Therefore , some logs that do not match the pattern of previous 
characteristics are considered anomalous . In the actual data ‐
set studied in this paper , log exceptions can be broadly classi ‐
fied into two categories : 1) Business exceptions caused by net ‐
work blocking , resource usage , etc. When such exceptions are generated , the program will actively retry the process , so some 
logs will be repeatedly generated several times in a short pe ‐
riod of time . 2) Exceptions triggered by service deployment or 
termination failure . This type of exception usually generates 
only a few error logs , which can be evidenced by the fact that 
the sequence of logs is abnormal , and the semantics of the er ‐
ror logs differ significantly from the normal logs .
2.2 Challenges and Analysis
1) Challenge 1 is modeling the normal patterns of logs . Tra‐
ditional anomaly detection algorithms always work on learning 
the normal patterns of logs and finding out logs different from 
normal patterns . Most studies mainly focus on building the 
normal pattern according to the sequence and frequency of 
logs, however , these two features are not comprehensive 
enough to evaluate the overall state of the system , hence the 
normal patterns built on these two features are not accu ‐
rate[3–4]. Intuitively , each log has its semantic characteristics 
through the log message , and these would describe the log 
meaning , such as inserting in a dataset , deleting from a data ‐
set, and failure reporting . Besides , logs are generated by trig ‐
gering corresponding events , and an event always triggers a se ‐
ries of logs . So, there is a sequential relation between logs , 
which would become different from the usual and could repre ‐
sent the status of logs and systems . As above , if we could com ‐
bine the semantic information and sequential message with 
other statistical information , it could perform better in detect ‐
ing anomalies in logs .
2) Challenge 2 is massive noises in the log . Logs are pro ‐
duced by a software system , which is highly concurrent and 
greatly influenced by the network . A working software system 
can run a large number of programs at the same time . These 
programs can produce a series of logs as well as useless mes ‐
sages , such as test output , and these messages have no effect 
on evaluating the log status . Some programs need to interact 
with other programs in the network , and thus the status of the 
network would affect the log sequence and delays can disrupt 
the order of logs . The disordered log and useless information 
are collectively called noise . Dealing with noise properly is 
necessary to improve anomaly detection in logs , and the sim ‐
plest way is to remove the noise , which , however , roughly 
brings lots of missing areas in logs and could bring new prob ‐
lems to the model . Based on the above observation , intuitively , 
if we can filter out the false positive alarms instead of roughly 
removing these noises , log anomaly detection can achieve 
higher accuracy and lower the false alarm rate .
2.3 Preliminaries
1) The log parsing algorithm . To parse the semi -structured 
log into a structured text , the log parsing algorithm focuses on 
fetching the unstructured part from the structured part and ex ‐
tracting the features of logs . Specifically , the log parsing algo ‐
rithm would replace meaningless information (e.g., IP address , 
▲Figure 1. Log instances , where black words are the structured part 
and red words are the unstructured part
71