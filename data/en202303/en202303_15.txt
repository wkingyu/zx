ZTE COMMUNICATIONS
September  2023  Vol.21 No.3FENG Bingyi , FENG Mingxiao , WANG Minrui , ZHOU Wengang , LI Houqiang Multi -Agent Hierarchical Graph Attention Reinforcement Learning for Grid -Aware Energy Management    Special Topic
generation device comprising solar cells .
The example of buildings and controllable components are 
shown at the top of Fig . 1. For instance , there are some build ‐
ings located on the third bus , and each building has the above 
four components to control to stabilize voltage after satisfying 
users’ energy demands . Energy demand , including the use of 
HVAC and DHW , and other electric equipment/appliances 
(non -shiftable loads ), as these components may constantly con ‐
sume electricity from the power grid .
In terms of constructing the power models , grid -aware en ‐
ergy management environment GridLearn[5], grid models and 
AC power flows , etc., are modeled using Pandapower . The 
Pandapower library models the loads of the buildings with real 
and apparent power specifications ; the PV arrays (and corre ‐
sponding inverters ) are modeled as PQ -controlled generators , 
which are defined to hold the active power P and reactive 
power Q constant while the voltage is allowed to vary over the 
limited range . It also calculates real and reactive power at 
each bus , load , and generator along with voltages at each bus . 
These values can be adapted to the state space or reward func ‐
tion. And they apply the preconfigured IEEE network model 
in it.
A large number of PV inside buildings will continuously in ‐
ject power into the power grid and the power grid also needs to 
supply power frequently to meet the various users ’ energy de ‐
mands , which may lead to frequent undervoltage or overvolt ‐
age problems in the power grid . Specifically , the voltage of 
each bus will be varied if it is injected with active power and 
reactive power . The exact numerical change of voltage is cal ‐
culated with these two types of power through certain power 
flow formulas in the power flow model[5]. The formulas with 
physical quantities in the distribution network are compli ‐
cated and non -linear in order to satisfy power system dynam ‐
ics regulations[2].
The traditional control techniques for large -scale , com ‐
plex, and non -linear systems are inadequate for real -time 
decision -making , particularly in systems with high penetra ‐
tion of renewable energy sources[11]. As a result , the employ ‐
ment of deep reinforcement learning algorithms has emerged 
as a potential and effective method in the literature to miti ‐
gate these difficulties .
2.2 MARL Formulations
The cooperative control process of grid -aware energy man ‐
agement can be modeled as a decentralized partially observ ‐
able Markov decision process (DEC -POMDP )[15]. A DEC -
POMDP is an extension of an MDP in decentralized multi -
agent settings with partial observability . It can be defined by 
S, A, O, R, P, N, γ, where S is the state space , Ai is the ac ‐
tion space for agent i, oi=O(s;i) is the local observation for 
agent i at global state s, P(s'|s, A) denotes the transition prob ‐
ability from  S to S' given the joint action A=(a1,…,an) for all 
N agents , R(s, A) is the shared reward function and can also be called a global reward function , and γ∈[0,1) is the dis ‐
count factor . In a DEC -POMDP , each agent takes observation 
from the environment and executes an action generated by its 
policy to the environment . In turn , the environment provides 
one global feedback reward to all agents . During the interac ‐
tion with the environment , the agents constantly adjust their 
policies to achieve the best decisions according to the re ‐
wards . Considering the grid -aware energy management prob ‐
lem, we describe specific elements in the DEC -POMDP in de ‐
tail as follows , similar to Ref . [5].
Agent : As shown in Fig . 1, each building is regarded as an 
agent and will make control decisions on four components to 
maintain the voltage of all buses within a safe range .
Observation : The agent ’s observation incorporates 18 state 
spaces such as outdoor temperature , indoor temperature , volt‐
age magnitude at the located bus , electricity generated by pho ‐
tovoltaic current , electricity consumed by base loads , current 
energy demand , time of day and the charging states of an 
HVAC storage device , a DHW storage device , and a battery .
Action : Each building controls four components , namely  
HVAC energy storage , DHW energy storage , battery storage , 
and inverters . The action made on each component is continu ‐
ous and is all set in range [-1,1]. For the three energy storage 
components , the action denotes the increase (action> 0) or de ‐
crease (action< 0) of the energy ’s rate stored in the correspond ‐
ing storage device . For the inverter , the action made on the in ‐
verter is used to scale the active power and reactive power sup ‐
plied by PV and the battery .
Reward function : The reward function is mainly based on 
the voltage deviation from 1 p.u. for each bus . The term p .u. 
referred to “per unit” is used to express the voltage level in 
terms of a percentage of the nominal voltage . To alleviate the 
overvoltage and undervoltage problem across all buses , the re ‐
ward function is calculated through the voltages on all buses . 
Specifically , let B denote the set of all buses in the distribu ‐
tion network , vi denote the voltage on i’s bus , and δi a weight ‐
ing factor to approximately normalize the reward function . The 
global reward function is calculated as follows :
R=-∑
i∈B(δi(vi-1))2
. (1)
Note that this function limits the reward to 0 or negative 
and is devised to penalize the voltage rise deviation and the 
voltage drop deviation from 1 p.u. followed by Ref . [5]. Volt ‐
age deviations are typically measured from 1 p.u. (or 100% 
of the nominal voltage ). For instance , if a 4% voltage devia ‐
tion is allowed , the voltage safe range is from 0.96 p.u. to 
1.04 p.u.
2.3 Centralized Training Decentralized Execution
Centralized training and decentralized execution (CTDE ) is 
one of the paradigms in MARL which assumes that global in ‐
13